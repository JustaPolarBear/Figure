# CHAPTER 7 Attention

A PATI ENT WHO suffered a severe stroke several weeks ago sits with his wife as she talks with his neurologist. At first it seemed that the stroke had left him totally blind, but his wife states that he can sometimes see things, and that they are hoping his vision will improve. The neurologist soon realizes that although her patient does have serious visual problems, he is not completely blind. Taking a comb from her pocket, the doctor holds it in front of her patient and asks him, 

A PATIENT WHO suffered a severe stroke several weeks ago sits with his wife as she talks with his neurologist. At first it seemed that the stroke had left him totally blind, but his wife states that he can sometimes see things, and that they are hoping his vision will improve. The neurologist soon realizes that although her patient does have serious visual problems, he is not completely blind. Taking a comb from her pocket, the doctor holds it in front of her patient and asks him, "What do you see?" (Figure 7.1a).

“Well, I’m not sure,” he replies, “but . . . oh . . . it’s a comb, a pocket  comb.”

“Good,” says the doctor. Next she holds up a spoon and asks the  same question (Figure 7.1b).

After a moment the patient replies, “I see a spoon.”

The doctor nods and then holds up the spoon and the comb together. “What do you see now?” she asks.

He hesitantly replies, “I guess . . . I see a spoon.”

“Okay . . . ,” she says, as she overlaps the spoon and comb in a  crossed fashion so they are both visible in the same location. “What do  you see now?” (Figure 7.1c). Oddly enough, he sees only the comb. “What about  a spoon?” she asks.

“Nope, no spoon,” he says, but then suddenly blurts out, “Yes, there it is, I see  the spoon now.” 

“Anything else?” 

Shaking his head, the patient replies, “Nope.” 

Shaking the spoon and the comb vigorously in front of her patient’s face, the doctor persists, “You don’t see anything else, nothing at all?”

He stares straight ahead, looking intently, and finally says, “Yes . . . yes, I see  them now . . . I see some numbers.” 

“What?” says the puzzled doctor. “Numbers?”

"Yes." He squints and appears to strain his vision, moving his head ever so slightly, and replies, “I see numbers." The doctor then notices that the man's gaze is directed to a point beyond her and not toward the objects she is holding. Turning to glance over her own shoulder, she spots a large clock on the wall behind her!

Even though the doctor is holding both objects in one hand directly in front of her patient, overlapping them in space and in good lighting, he sees only one item at a time. That one item may even be a different item altogether: one that is merely in the direction of his gaze, such as the clock on the wall. The patient can see" each of the objects presented by the doctor, but I he fails to see them all together and cannot accurately describe their locations with respect to each other or to himself. From these symptoms the neurologist diagnoses the patient with Bálint’s syndrome, a condition caused by bilateral damage to regions of the posterior parietal and occipital cortex. The result is a severe disturbance of visual attention and awareness, in which only one or a small subset of available objects is perceived at any one time and is mislocalized ins ace.

Bálint's syndrome is an extreme pathological instance of what we all experience daily: We are consciously aware of only a small bit of the vast amount of information available to our sensory systems from moment to moment. By looking closely at patients with attentional problems, we have come to learn more about how, and on what, the brain focuses attention. The central problem in the study of attention is the question of how the brain is able to select some
information at the expense of other information.

Robert Louis Stevenson wrote, "The world is so full of a number of things, I'm sure we should all be as happy as kings." Although those things may make us happy, the sheer number of them presents a problem to our perception system: information overload. We know from experience that we are surrounded by more information than we can handle and comprehend at any given time. The nervous system, therefore, has to make "decisions" about what to process. Our survival may depend on which stimuli are selected and in what order they are prioritized for processing. In this chapter, after describing what is meant by attention and reviewing the anatomical structures involved with it, we consider how damage to the brain changes human attention and gives us insight into how attention is organized in the brain. Then we discuss how attention influences sensation and perception. We conclude with a discussion of the brain networks used for attentional control.

# 7.1 Selective Attention and  the Anatomy of Attention

At the end of the 19th century, the great American psychologist William James (Figure 7.2) made an astute  observation:

> Everyone knows what attention is. It is the taking possession by the mind, in clear and vivid form, of one out of what seem several simultaneously possible objects or trains of thought. Focalization, concentration of consciousness are of its essence. It implies withdrawal from some things in order to deal effectively with others, and is a condition which has a real opposite in the confused, dazed, scatterbrain state. (W. James, 1890)

James insightfully captured key characteristics of attentional phenomena that are under investigation today. "It is the taking possesSion by the mind" suggests that we can choose the focus of attention; that is, it can be voluntary. His mention of "one out of what seem several simultaneously possible objects or trains of thought" refers to the inability to attend to many things at once, and hence the selective aspects of attention. James raised the idea of limited capacity in attention by noting that "it implies withdrawal from some I things in order to deal effectively with others."

As clear and articulate as James's writings were, little was known about the behavioral, computational, or neural mechanisms of attention during his lifetime. Since then, knowledge about attention has blossomed, and researchers have identified multiple types and levels of attentive behavior. First, let's distinguish selective attention from arousal. Arousal refers to the global physiological and psychological state of the organism, and it is best thought of on a continuum ranging from deep sleep to hyperalertness (such as during periods of intense fear). We will discuss this further in Chapter 14.

In contrast, selective attention is not a global brain state. Instead, at any level of arousal, it is the allocationl of attention among relevant inputs, thoughts, and actions I while simultaneously ignoring irrelevant or distracting ones. Selective attention is the ability to prioritize and attend to some things and not others. What determines the priority? Many things. For instance, an optimal strategy in many situations is to attend to stimuli thatl are relevant to current behavior and goals. For example, to survive this class, you need to attend to this chapterl rather than your social media feed.

This is goal-driven control (also called top-down control), steered by an individual's current behavioral goals and shaped by learned priorities based on personal experience and evolutionary adaptations. Still, if you hear a loud bang, even while dutifully attending to this book, you will reflexively pop up your head and check it out. That is good survival behavior because a loud noise may presage danger. Your reaction is stimulus driven and is therefore termed stimulus-driven control (also known as bottom-up control), which is much less dependent on current behavioral goals.

The mechanisms that determine where and on what our attention is focused are referred to as attentional control mechanisms, and they involve widespread but highly specific cortical and subcortical networks that interact so that we can selectively process information (see the "Anatomical Orientation" box). Several cortical areas are important in attention: portions of the superior frontal cortex, posterior parietal cortex, and posterior superior temporal cortex, as well as more medial brain structures, including the anterior cingulate cortex. The superior colliculus in the midbrain and the pulvinar nucleus of the thalamus, located between the midbrain and the cortex, are involved in the control of attention.

We know that damage to these structures can lead to deficits in the ability to orient both overt attention (i.e., eye gaze direction) and covert attention (i.e., attention directed without changes in eye, head, or body orientation). Finally, attention acts on sensory systems, and therefore much work on attention investigates the effect of attention on sensory signal processing.

Attentional control mechanisms influence specific stages of information processing, where it is said that "selection" of inputs (or outputs) takes place–hence the term selective attention. (As shorthand, we will often just use the term attention when referring to the more specific concept of selective attention.) Attention influences how we code sensory inputs, store that information in memory, process it semantically, and act on it to survive in a challenging world. This chapter focuses on the mechanisms of selective attention and its role in perception and awareness.

TAKE-HOME MESSAGES Selective attention is the ability to focus awareness on one stimulus, thought, or action while ignoring other, irrelevant stimuli, thoughts, and actions.

- Arousal is a global physiological and psychological brain state, whereas selective attention describes what we attend and ignore within any specific level (high versus low) of arousal. Specific networks for the control of attention include cortical and subcortical structures. 
- Attention influences how we process sensory inputs, store that information in memory, process it semantically, and act on it.
- Specific networks for the control of attention include cortical and subcortical structures.
- Attention influences how we process sensory inputs, store that information in memory, process it semantically, and act on it.

---

# 7.2 The Neuropsychology of Attention

Much of what neuroscientists know about brain attention systems has been gathered from examinations of patients who have brain damage that influences attentional behavior. Many disorders result in deficits in attention, but only a few provide clues to which brain systems are being affected. Though the best-known disorder of attention, attention deficit hyperactivity disorder (ADHD), has heterogeneous genetic and environmental risk factors, it is characterized by disturbances in neural processing that may result from anatomical variations of white matter throughout the attention network. Structural MRI studies of ADHD patients have found decreased white matter volume throughout the brain, especially the prefrontal cortex (see Bush, 2010). The portions of the brain's attention networks affected by ADHD are yet to be fully identified.

In contrast, neuroscientists have derived more information about attentional control mechanisms, and the underlying neuroanatomical systems supporting attention, by investigating classic syndromes like unilateral spatial neglect (described next) and Bálint's syndrome. Because these disorders are the result of focal brain damage (e.g., stroke), they can be mapped in postmortem analyses and localized with brain imaging in the living human. Let's consider how brain damage has helped us understand these mechanisms.

## Neglect

A patient with neglect may notice you more easily when you are on her right side, comb her hair only on the right, eat from only the right half of her plate, and read only the right-hand page of a book. What's more, she may deny having any problems. Depending on the extent and location of her lesion, she may show all or a subset of these symptoms.

Unilateral spatial neglect, or simply neglect, is quite common. It results when the brain's attention network is damaged in one hemisphere, typically as the result of a stroke. More severe and persistent effects occur when the right hemisphere is damaged. As in the hypothetical patient just described, a right-hemisphere lesion biases attention toward the right, resulting in a neglect of what is going on in the left visual field. The patient behaves as though the left regions of space and the left parts of objects simply do not exist, and has limited or no awareness of her lesion and deficit.

The late German artist Anton Räderscheidt suffered a stroke in the right hemisphere at age 67, resulting in left-sided neglect. In a self-portrait done shortly after his stroke (Figure 7.3a), the entire left half of the canvas is largely untouched, and the left half of his face is missing. Over the following several months, Räderscheidt progressively used more of the canvas and included more of his face in his portraits (Figure 7.3b and c), until finally (Figure 7.3d), he used most of the canvas and had a bilaterally symmetrical face, although some minor asymmetries persisted.

Despite having normal vision, patients with neglect exhibit deficits in attending to and acting in the direction opposite the side of the unilateral brain damage. This phenomenon can be observed in their patterns of eye movements (Figure 7.4): These patterns, during rest and during a bilateral visual search in patients with a righthemisphere lesion and left-sided neglect (Figure 7.4a), can be compared to those of patients with right-hemisphere strokes without neglect (Figure 7.4b). The neglect patients show a pattern of eye movements biased in the direction of the right visual field, while those without neglect search the entire array, moving their eyes equally to the left and right (Corbetta & Shulman, 2011).

NEUROPSYCHOLOGICAL TESTS OF NEGLECT Neuropsychological tests are used to diagnose neglect. In the line cancellation test, patients are given a sheet of paper containing many horizontal lines and are asked to bisect them in the middle. Patients with left-sided neglect tend to bisect the lines to the right of the midline. They may also completely miss lines on the left side of the paper (Figure 7.5). The pattern of line cancellation is evidence of neglect at the level of object representations (each line) as well as visual space (the visual scene represented by the test paper).

A related test is to copy objects or scenes. Figure 7.6 shows an example from a patient with a right-hemisphere stroke who was asked to copy a clock. Like the artist Räderscheidt, the patient showed an inability to draw the entire object and tended to neglect the left side. Even when such patients know and can state that clocks are round and include the numbers 1 to 12, they cannot properly copy the image. Moreover, they cannot draw it from memory.

Neglect can also affect the imagination and memory. In Milan, Italy, Eduardo Bisiach and Claudio Luzzatti (1978) studied patients with neglect caused by unilateral damage to the right hemisphere. They asked the participants to imagine themselves standing on the steps of the Duomo di Milano (the Milan Cathedral) and to describe from memory the piazza from that viewpoint. Amazingly, the patients neglected things on the side of the piazza contralateral to their lesion, just as if they were actually standing there looking at it. Furthermore, when imagining themselves standing across the piazza, facing toward the Duomo, they reported items from visual memory that they had previously neglected, and they neglected the side of the piazza that they had just described (Figure 7.7).

Thus, neglect is found for items in visual memory during remembrance of a scene, as well as for items in the external sensory world. The key point in Bisiach and Luzzatti's experiment is that the patients' neglect could not be attributed to a failure of memory, but rather indicated that attention to parts of the recalled images was biased.

EXTINCTION Visual field testing shows that neglect patients are not blind in their left visual field: They are able to detect stimuli normally when those stimuli are salient and presented in isolation. For example, when simple flashes of light or the wiggling fingers of a neurologist are shown at different single locations within the visual field of a neglect patient, the patient can see each stimulus (Figure 7.8a and b). This result tells us that the patient does not have a primary visual deficit. The patient's neglect becomes obvious, however, when he is presented simultaneously with two stimuli, one in each hemifield. In that case, the patient fails to perceive or act on the contralesional stimulus (Figure 7.8c). This result is known as extinction, because the presence of the competing stimulus in the ipsilateral hemifield prevents the patient from detecting the contralesional stimulus.

These biases against the contralesional sides of space and objects can be overcome if the patient's attention is directed to the neglected locations of items. This is one reason the condition is described as a bias, rather than a loss of the ability to focus attention contralesionally. One patient's comments help us understand how these deficits might feel subjectively: "It doesn't seem right to me that the word neglect should be used to describe it. I think concentrating is a better word than neglect. It's definitely concentration. If I am walking anywhere and there's something in my way, if I'm concentrating on what I'm doing, I will see it and avoid it. The slightest distraction and I won't see it" (Halligan & Marshall, 1998).

## Comparing Neglect and Bálint’s Syndrome

Let's compare the pattern of deficits in neglect with those of the patient with Bálint’s syndrome described at the beginning of the chapter. In contrast to the patient with neglect, a Bálint’s patient demonstrates three main deficits that are characteristic of the disorder:

- Simultanagnosia is a difficulty in perceiving the visual field as a whole scene, such as when the patient saw only the comb or the spoon, but not both at the same time (Figure 7.1).
- Ocular apraxia is a deficit in making eye movements (saccades) to scan the visual field, resulting in the inability to guide eye movements voluntarily: When the physician overlapped the spoon and comb in space as in Figure 7.1c, the Bálint’s patient should have been able, given his direction of gaze, to see both objects, but he could not.
- Optic ataxia is a problem in making visually guided hand movements: If the doctor had asked the Bálint’s patient to reach out and grasp the comb, the patient would have had a difficult time moving his hand through space toward the object.

The patterns of perceptual deficits in neglect and Bálint’s syndrome are quite different, however, because different brain areas are damaged in each disorder. Neglect is the result of unilateral lesions of the parietal, posterior temporal, and frontal cortex, and it can also be due to damage in subcortical areas including the basal ganglia, thalamus, and midbrain. In contrast, Bálint’s patients suffer from bilateral occipitoparietal lesions. Neglect shows us that disruption of a network of cortical and subcortical areas, especially in the right hemisphere, results in disturbances of spatial attention. Bálint’s syndrome shows us that posterior parietal and occipital damage to both hemispheres leads to an inability to perceive multiple objects in space, which is necessary for creating a scene.

From patients with neglect, we understand that the symptoms involve biases in attention based on spatial coordinates, either with respect to the patient (egocentric reference frame) or with respect to an object in space (allocentric reference frame). This finding tells us that attention can be directed within space and also within objects. Most likely these two types of neglect are guided by different processes. Indeed, the brain mechanisms involved with attending objects can be affected even when no spatial biases are seen. This phenomenon is evident in patients with Bálint's syndrome, who have relatively normal visual fields but cannot attend to more than one or a few objects at a time, even when the objects overlap in space.

The phenomenon of extinction in neglect patients suggests that sensory inputs are competitive, because when two stimuli presented simultaneously compete for attention, the one in the ipsilesional hemifield wins the competition and reaches the patient's awareness. Extinction also demonstrates that after brain damage, patients experience reduced attentional capacity: When two competing stimuli are presented at once, the neglect patient is aware of only one of them.

These observations from brain damage and resultant attentional problems set the stage for us to consider several questions:

- How does attention influence perception?

- Where in the perceptual system does attention influence perception? 
- What neural mechanisms control what we attend? 

To answer these questions, let's look next at the cognitive and neural mechanisms of attention.

## TAKE-HOME MESSAGES

- Unilateral spatial neglect may result from damage to the right parietal, temporal, or frontal cortices, as well as to subcortical structures. This kind of damage leads to reduced attention to and processing of the left-hand side of scenes and objects, not only in external personal hemispace but also in internal memory.
- Neglect is not the result of sensory deficits, because visual field testing shows that these patients have intact vision. Under the right circumstances, they can easily see objects that are sometimes neglected.
- A prominent feature of neglect is extinction, the failure to perceive or act on stimuli contralateral to the lesion (contralesional stimuli) when presented simultaneously with a stimulus ipsilateral to the lesion (ipsilesional stimulus). 
- Patients with Bálint’s syndrome have three main deficits characteristic of the disorder: difficulty perceiving the visual field as a whole scene, an inability to guide eye movements voluntarily, and difficulty reaching to grab an object.

---

# 7.3 Models of Attention

Attention can be divided into two main forms: voluntary attention and reflexive attention. Voluntary attention, also known as endogenous attention, is our ability to intentionally attend to something, such as this book. It is a top-down, goal-driven process, meaning that our goals, expectations, and rewards guide what we attend. Reflexive attention, also referred to as exogenous attention, is a bottom-up, stimulus-driven process in which a sensory event–maybe a loud bang, the sting of a mosquito, a whiff of garlic, a flash of light, or motion–captures our attention.

These two forms of attention can be thought of as opposing systems–one supporting our ability to focus attention to achieve momentary behavioral goals, and the other being driven by the world around us. It is useful to think of these two attention systems as being in balance, so that we are neither so focused on something like a beautiful flower that we miss the tiger sneaking up behind us, nor so distracted by environmental sights and sounds that we attend to flashing billboards when we need to concentrate on driving in heavy traffic. As we will see later in this chapter, these two forms of attention differ in their properties and partly in their neural mechanisms.

Attention orienting also can be either overt or covert. We all know what overt attention is: When you turn your head to orient toward a stimulus–whether it is for your eyes to get a better look or your ears to pick up a whisper–you are exhibiting overt attention. However, you could appear to be reading this book while actually paying attention to the two students whispering at the table behind you. This behavior is covert attention. Much of attention research focuses on understanding covert attention mechanisms, because these necessarily involve changes in internal neural processing and not merely the aiming of sense organs to better pick up information.

## Hermann von Helmholtz and Covert Attention

In 1894, Hermann von Helmholtz was investigating aspects of the visual processing of briefly perceived stimuli. He constructed a screen on which letters were painted at various distances from the center (Figure 7.9). He hung the screen at one end of his lab, turned off all the lights to create a completely dark environment, and then used an electrical spark to make a flash of light that briefly illuminated the screen. As often happens in science, he stumbled onto an interesting phenomenon.

Helmholtz noted that the screen was too large to view without moving his eyes. Nonetheless, even when he kept his eyes fixed on the center of the screen, he could decide in advance where he would pay attention using covert attention. As noted earlier, coven means that the location toward which he directed his attention could be different from the location toward which he was looking. Through these covert shifts of attention, Helmholtz observed that during the brief period of illumination, he could perceive letters located within the focus of his attention better than letters that fell outside the focus of his attention, even when his eyes remained directed toward the center of the screen.

> These experiments demonstrated, so it seems to me, that by a voluntary kind of intention, even without eye movements, and without changes of accommodation, one can concentrate attention on the sensation from a particular part of our peripheral nervous system and at the same time exclude attention from all other parts. (Helmholtz, 1909–11/1968)

In the mid 20th century, experimental psychologists began to develop methods for quantifying the influence of attention on perception and awareness. Models of how the brain's attention system might work were built from these data and from observations like those of Helmholtz–as well as from everyday experiences, such as attending a crowded party.

## The Cocktail Party Effect

Imagine yourself at a Super Bowl party having a conversation with a friend. How can you focus on this single conversation while the TV is blaring and boisterous conversations are going on around you? British psychologist E. C. Cherry (1953) wondered the same thing while attending cocktail parties. His curiosity and subsequent research helped to found the modern era of attention studies with what was dubbed the cocktail party effect.

Selective auditory attention enables you to participate in a conversation at a busy restaurant or party while ignoring the rest of the sounds around you. By selectively attending, you can perceive the signal of interest amid the other noises. If, however, the person you are conversing with is boring, then you can give covert attention to a conversation going on behind you while still seeming to focus on the conversation in front of you (Figure 7.10).

Cherry investigated this ability by designing a cocktail party in the lab–the first use of the dichotic listening task described in Chapter 4. Normal participants, wearing headphones, listened to competing speech inputs to the two ears. Cherry then asked the participants to attend to and verbally "shadow" the speech (i.e., immediately repeat each word) coming into one ear while simultaneously ignoring the input to the other ear. Cherry discovered that under such conditions, for the most part, participants could not report any details of the speech in the unattended ear (Figure 7.11). In fact, all they could reliably report from the unattended ear was whether the speaker was male or female. Attention–in this case voluntary attention–affected what was processed.

This finding led Cherry and others to propose that attention to one ear results in better encoding of the inputs to the attended ear and loss or degradation of the unattended inputs to the other ear. You experience this effect when the person sitting next to you in lecture whispers a juicy tidbit in your ear. A moment later, you realize that you missed what the lecturer just said, although you could easily have heard him with your other ear. As foreshadowed by William James, bottlenecks in information processing–stages through which only a limited amount of information can pass–seem to occur at stages of perceptual analysis that have a limited capacity. There are many stages of processing between the time information enters your eardrum and the time you become aware of what was said. At which stages are there bottlenecks that make attention necessary to favor one signal over another?

This question has led to one of the most debated issues in psychology over the past six decades: Are the effects of selective attention evident early in sensory processing or only later, after sensory and perceptual processing are complete? Does the brain faithfully process all incoming sensory inputs to create a representation of the external world, or can processes like attention influence sensory processing? Is your perception of the external world biased by the current goals and stored knowledge of your internal world? Consider the example in Figure 7.12. What images do you see?

The first time you looked at this image, you probably did not see the Dalmatian; you could not perceive it easily. Once the dog is pointed out to you, however, you perceive it whenever you are shown the picture. Something has changed in your brain, and it is not simply knowledge that it is a photo of a dog; the dog jumps out at you, even when you forget having seen the photo before. This is an example of stored knowledge influencing your perception. Perhaps it is not either-or; it may be that attention affects processing at many steps along the way from sensory transduction to awareness.

## Early-Selection Models Versus Late-Selection Models

Cambridge University psychologist Donald Broadbent (1958) elaborated on the idea that the informationprocessing system (i.e., the totality of neural processing that stimulus inputs or actions undergo in the brain) has processing bottlenecks (Figure 7.13). In Broadbent's model, the sensory inputs that can enter higher levels of the brain for processing are screened early in the information-processing stream by a gating mechanism so that only the "most important," or attended, events pass through. Early selection is the idea that a stimulus can be selected for further processing or be tossed out as irrelevant before perceptual analysis of the stimulus is complete. In contrast, models of late selection hypothesize that the perceptual system first processes all inputs equally, and then selection takes place at higher stages of information processing that determine whether the stimuli gain access to awareness, are encoded in memory, or initiate a response. Figure 7.14 illustrates the differential stages of early versus late selection.

The original all-or-none early-selection models quickly ran into a problem. Cherry observed in his cocktail party experiments that salient information from the unattended ear was sometimes consciously perceived–for example, when the listener's own name or something very interesting was included in a nearby conversation. The idea of a simple gating mechanism, which assumed that ignored information was completely lost, could not explain this experimental finding.

Anne Treisman (1969) proposed that information from an unattended channel was not completely blocked from higher analysis but was degraded or attenuated instead–a point that Broadbent agreed with. Thus, early-selection versus late-selection models were modified to make room for the possibility that information in the unattended channel could reach higher stages of analysis, but with greatly reduced signal strength. To test these competing models of attention, researchers employed increasingly sensitive methods for quantifying the effects of attention, as described next.

## Quantifying the Role of Attention in Perception

One way of measuring the effect of attention on information processing is to examine how participants respond to target stimuli under differing conditions of attention. One popular method is to provide cues that direct the participant's attention to a particular location or target feature before presenting the task-relevant target stimulus. In these so-called cuing tasks, the focus of attention is manipulated by the information in the cue.

In cuing studies of voluntary spatial attention, University of Oregon professor Michael Posner and his colleagues presented participants with a cue that directs their attention to one location on a computer screen (Figure 7.15). Next, a target stimulus is flashed onto the screen at either the cued location or another location. Participants may be asked to press a button as fast as they can following the presentation of a target stimulus to indicate that it occurred, or they may be asked to respond to a question about the stimulus, such as "Was it red or blue?" The design of this study enables researchers to learn how long it takes to perform the task (reaction time, or response time), how accurately the participant performs the task (accuracy, or error rate), or both.

In one version of this experiment, participants are instructed that although the cue, such as an arrow, will indicate the most likely location of the upcoming stimulus, they are to respond to the target wherever it appears. The cue, therefore, predicts the location of the target on most trials (a trial is one presentation of the cue and subsequent target, along with the required response). This form of cuing is known as endogenous cuing, where the orienting of attention to the cue is voluntary and driven by the participant's goals (here, compliance with the instructions) and the meaning of the cue. In contrast, an exogenous cue automatically captures attention because of its physical features (e.g., a flash of light; see "Reflexive Visuospatial Attention" in Section 7.4 for this mechanism).

When a cue correctly predicts the location of the subsequent target, it is a valid trial (Figure 7.15a). If the relation between cue and target is strong–that is, the cue usually (say, 90% of the time) predicts the target location–then participants learn to use the cue to predict the next target's location. Sometimes, though, because the target may be presented at a location not indicated by the cue, the participant is misled in an invalid trial (Figure 7.15b). Finally, the researcher may include some cues that give no information about the most likely location of the impending target–a neutral trial (Figure 7.15c).

In cuing studies of voluntary attention, the time between presentation of the cue and the subsequent target is very brief: a few hundred milliseconds, or at most a second or two. When participants are not permitted to move their eyes to the cued location in space but instead must keep them fixed on a central fixation point (covert attention), and the cue correctly predicts the target's location, participants respond faster than when neutral cues are given (Figure 7.16). This faster response demonstrates the benefits of attention. In contrast, reaction times are longer (i.e., responses are slower) when the stimulus appears at an unexpected location, revealing the costs of attention. If participants are asked to discriminate a particular feature of the target, then the benefits and costs of attention can be expressed in terms of accuracy instead of, or in addition to, measures of reaction time.

These benefits and costs have been attributed to the influence of covert attention on the efficiency of information processing. According to most theories, a highly predictive cue induces participants to direct their covert attention internally, shining a sort of mental "spotlight" of attention onto the cued visual field location. The spotlight is a metaphor to describe how the brain may attend to a spatial location. Because participants are typically required to keep their eyes on a fixed spot different from the one to be attended, internal or covert mechanisms of attention must be at work.

Posner and his colleagues (1980) have suggested that this attentional spotlight affects reaction times by influencing sensory and perceptual processing; that is, stimuli that appear in an attended location are processed faster during perceptual processing than are stimuli that appear in an unattended location. This enhancement of attended stimuli, a type of early selection, is consistent with the proposal that changes in perceptual processing can happen when the participant is covertly attending a stimulus location.

Now, you might be thinking, "Wait a minute– responding more quickly to a target appearing at an attended location does not imply that the target was more efficiently processed in the sensory pathways." Indeed, measures of motor reaction time–or behavioral measures more generally–provide only indirect assessments of specific stages of neural processing. Differences in response time could solely reflect events going on after sensory processing was complete–for example, in the motor system. So, how can we be sure that the perceptual system is actually being affected by attention if our measure is the speed or accuracy of motor responses?

Ideally, this question would be resolved by direct measurement of different discrete stages of perceptual and post-perceptual processing, rather than by attempts to infer them from the behavioral responses. In order to determine whether changes in attention truly affect perceptual processing stages in the brain, researchers combined noninvasive recording methods with the experimental methods developed by cognitive psychologists, such as cuing paradigms. This combination of methods yielded the first definitive physiological evidence favoring early selection in humans, which came from studies using event-related potential (ERP) recordings to measure sensory processes during attention.

For example, neural signals elicited by attended sounds were strongly amplified in the auditory cortex starting at 50 ms after sound onset (Hillyard et al., 1973). In the visual modality, attended input signals were similarly enhanced (and unattended inputs suppressed) in extrastriate visual cortex starting at about 80 ms (Van Voorhis and Hillyard, 1977). The early timing of these effects of attention indicated that attentional selection occurs at early levels of sensory processing before the stimulus properties can be fully analyzed. In the next section we will look closely at the effects of attention on visual processing.

TAKE-HOME MESSAGES

- Attention involves both top-down (voluntary), goaldirected processes and bottom-up (reflexive), stimulusdriven mechanisms, and it can be either overt or covert.
- According to early-selection models, a stimulus need not be completely perceptually analyzed before it can be selected for further processing or rejected as irrelevant. Broadbent proposed such a model of attention. Late-selection models hypothesize that attended and ignored inputs are processed equivalently by the perceptual system, and that selection can occur only upon reaching a stage of semantic (meaning) encoding and analysis.
- Our perceptual system contains limited-capacity stages at which it can process only a certain amount of information at any given time, resulting in processing bottlenecks. Attention limits the information to only the most relevant, thereby preventing overload.
- Spatial attention is often thought of metaphorically as a "spotlight" of attention that can move around as the person consciously desires or can be reflexively attracted by salient sensory events.

---

# 7.4 Neural Mechanisms of Attention and Perceptual Selection

Although most of the experiments discussed in this chapter focus on visual attention, this should not be taken to suggest that attention is only a visual phenomenon. Selective attention operates in all sensory modalities. Nevertheless, in this chapter we focus on the visual system as the model system.

In this section we will explore how attention influences the sensory processing of incoming signals during both goal-directed and stimulus-drawn attention. We will discuss when (time course) and where (functional anatomy) attention influences incoming signal processing, using evidence from human and animal studies and describing both cortical and subcortical mechanisms. Finally, we will consider the neural mechanisms underlying attentional selection of simple stimulus features such as color and location, or higher-order features such as objects, when we are searching for something or someone in a complex scene, like a friend in a busy airport terminal.

## Voluntary Visuospatial Attention

Visuospatial attention involves selecting a stimulus on the basis of its spatial location. It can be voluntary, such as when you attend to this page, or it can be reflexive, such as when motion at the door of the classroom attracts your attention and you look up. Effects of visuospatial attention can be seen in both the cortex and the subcortex.

CORTICAL ATTENTION EFFECTS Neural mechanisms of visuospatial attention have been investigated using cuing methods combined with ERP recordings. In a typical experiment, participants are given instructions to covertly attend to stimuli presented at one location (e.g., the right field) and to ignore stimuli presented at another location (e.g., the left field) while ERP recordings are made (Figure 7.17a). Recall that ERPs are the brain signals evoked by a stimulus, neural event, or motor response that can be extracted using signal averaging from the ongoing EEG.

A typical sensory-evoked ERP recording from a stimulus in one visual field shows a series of positive and negative voltage deflections in the first couple hundred milliseconds after the onset of the stimulus, whether the stimulus is attended or ignored. These brain responses are evoked by the physical features of the stimulus and represent cortical stimulus processing. The first big ERP wave has a positive polarity, begins following a latency period of 60 to 70 ms after stimulus onset, and peaks at about 100 ms over the occipital cortex contralateral to the visual hemifield of the stimulus. It is often referred to as the Pl component (P for positive polarity, and 1 because it is the first big wave; Figure 7.17b). It is followed by a negative wave that peaks at about 180 ms (N 1), and then by a series of positive and negative waves (P 2, N2, etc.).

Early studies revealed that attention modulates the amplitudes of these sensory-evoked ERPs, beginning with the Pl wave (Van Voorhis & Hillyard, 1977). When a visual stimulus appears at a location to which a participant is attending, the Pl wave is larger in amplitude (Figure 7.17b, solid red line) than when the same stimulus appears at the same location but attention is focused elsewhere (Figure 7.17b, dashed blue line). The same is true for the attention effects observed in studies of auditory and tactile selective attention, where the auditory and tactile sensory ERPs are larger when participants attend the stimulus.

Where within the visual sensory hierarchy (see Figure 5.26) do these earliest effects of selective visuospatial attention take place, and what do they represent? Various pieces of evidence are needed to answer this question, including the time course of the amplitude differences with attention. We've learned from intracranial recordings in human patients that the first volleys of afferent inputs into striate cortex (V1) take place with a latency longer than 35 ms, and that early visual cortical responses are in the same latency range as the Pl response (60–100 ms).

Taken together, these clues suggest that the Pl wave is a sensory wave generated by neural activity in the visual cortex and that, therefore, its sensitivity to spatial attention supports early-selection models of attention. We know from Chapter 3 that ERPs represent the summed electrical responses (post-synaptic potentials) of tens of thousands of neurons, not single neurons. This combined response produces a large enough signal to propagate through the skull to be recorded on the human scalp. But can the effect of attention be detected in the response of single visual neurons in the cortex?

Jeff Moran and Robert Desimone (1985) revealed the answer to this question. They investigated how selective visuospatial attention affected the firing rates of individual neurons in the visual cortex of monkeys. The researchers trained the monkeys to fixate on a central spot on a monitor, to covertly attend to the stimulus at one location in the visual field, and to perform a task related to it while ignoring the other stimulus. Using single-neuron recordings, they first characterized the responses of single neurons in extrastriate visual area V4 to figure out which regions of the visual field were sending them information (i.e., what their receptive field was; see Chapter 5) and which specific stimulus features the neurons responded to most vigorously.

The team found, for example, that neurons in V4 fired robustly in response to one stimulus with a particular color and orientation (e.g., a red, horizontal bar) more than another (e.g., a green, vertical bar). Next, they simultaneously presented the preferred (red, horizontal) and non-preferred (green, vertical) stimuli near each other in space, so that both stimuli were within the neuron's receptive field.

Responses of single neurons were recorded and compared under two conditions: when the monkey attended the preferred (red, horizontal bar) stimulus at a specific location, and when it instead attended the non-preferred (green, vertical bar) stimulus that was located a short distance away. Because the two stimuli (attended and ignored) were positioned in different locations, the task can be characterized as a spatial attention task. How did attention affect the firing rate of the neurons?

When the red stimulus was attended, it elicited a stronger response (more action potentials fired per second) in the corresponding V4 neuron that preferred red, horizontal bars than when the red stimulus was ignored and the green, vertical bar positioned at another location was attended. Thus, selective spatial attention affected the firing rates of V4 neurons (Figure 7.18). As with the ERPs in humans, the activity of single visual cortical neurons was found to be modulated by spatial attention.

Several studies have replicated the attention effects observed by Moran and Desimone in area V4 and have extended this finding to other visual areas, including later stages of the ventral pathway in the inferior temporal (IT) cortex. In addition, work in dorsal-stream visual areas has demonstrated effects of attention in the motion-processing areas MT and MST of the monkey. Researchers have also investigated whether attention affected even earlier steps in visual processing–in primary visual cortex (V1), for example.

Carrie McAdams and Clay Reid at Harvard Medical School (2005) carried out experiments to determine which level of processing within V1 was influenced by attention. Recall from Chapters 5 and 6 that many stages of neural processing take place within a visual area, and that in V1, different neurons display characteristic receptive-field proper ties; some are called simple cells, others complex cells, and so on. Simple cells exhibit orientation tuning and respond to contrast borders (like those found along the edge of an object). Simple cells are also situated and active relatively early in the hierarchy of neuronal processing in V1–so, if attention were to affect them, this would be further evidence of how early in processing, and by what mechanism, spatial attention acts within V1.

McAdams and Reid trained monkeys to fixate on a central point and covertly attend a black-and-white flickering noise pattern in order to detect a small, colored pixel that could appear anywhere within the pattern (Figure 7.19a). The monkeys were to signal when they detected the color by making a rapid eye movement (a saccade) from fixation to the location on the screen that contained that color. The attended location would be positioned either over the receptive field of the V1 neuron being recorded or in the opposite visual field. Thus, the researchers could evaluate responses of the neuron when that region of space was attended and when it was ignored (in different blocks).

They also could use the flickering noise pattern to create a spatiotemporal receptive-field map (Figure 7.19b) showing primary or secondary subregions of the receptive field that were either excited or inhibited by light. In this way, the researchers could determine whether the neuron had the properties of simple cells and whether attention affected the firing pattern and receptive-field organization. They found that spatial attention enhanced the responses of the simple cells (Figure 7.19c) but did not affect the spatial or temporal organization of their receptive fields, which remained unchanged over the trials.

So, from monkey cellular recordings it is clear that attention affects processing at multiple stages in the cortical visual pathways from V1 to IT cortex. Neuroimaging studies of spatial attention in humans show results consistent with the findings in monkeys, and they have the advantage of being able to measure the influence of attention on cortical visual processing in multiple different visual areas at one time. Work in humans also offers the opportunity to compare the information from functional imaging to the ERP findings in humans that were described earlier.

In an early study, Hans-Jochen Heinze and his colleagues (1994) directly related ERP findings to functional brain neuroanatomy by combining positron emission tomography (PET) imaging with ERP recordings. They demonstrated that visuospatial attention modulates the blood flow in visual cortex and that these hemodynamic changes could be related to the ERP effects observed on the Pl wave of the ERP. These findings suggested that the Pl and the attention effects on Pl (see Figure 7.17) took place in early extrastriate cortex. Subsequent studies using functional MRI have permitted a more fine-grained analysis of the effects of spatial attention in humans.

For example, Joseph Hopfinger and his colleagues (2000) used a modified version of a spatial cuing task combined with event-related fMRI. On each trial, an arrow cue was presented at the center of the display and indicated the side to which participants should direct their attention. Eight seconds later, the bilateral target display (flickering black-and-white checkerboards) appeared for 500 ms. The participants' task was to press a button if some of the checks were gray rather than white, but only if the gray target appeared on the cued side.

The 8-second gap between the arrow and the target display allowed the slow hemodynamic responses linked to the attention-directing cues to be analyzed separately from the hemodynamic responses linked to the detection of and response to the target displays. The results, shown in Figure 7.20 in coronal sections through the visual cortex of a single participant in the Hopfinger study, indicate that attention to one visual hemifield activated multiple regions of visual cortex in the contralateral hemisphere.

Roger Tootell, Anders Dale, and their colleagues at Massachusetts General Hospital (Tootell et al., 1998) used fMRI to create retinotopic maps to investigate how all of the attention-related activations in visual cortex related to the multiple visual cortical areas in humans. To differentiate and identify one activated visual area from another on the scans, they combined high-resolution mapping of the borders of early visual cortical areas (Figure 7.21a) with a spatial attention task. Participants were required to selectively attend to stimuli located in one visual field quadrant while ignoring those in the other quadrants; different quadrants were attended to in different conditions while the participants' brains underwent fMRI scanning.

The researchers were thus able to map the sensory responses to target stimuli (Figure 7.21b), and the attention-related modulations of these sensory responses onto flattened computer maps of the visual cortex, thereby directly relating the attention effects to the multiple visual cortical areas (Figure 7.21c and d). They found that spatial attention increased activity in multiple visual areas in the cortical regions coding the attended target locations, but not in regions that were ignored. This work provides a high-resolution view of the functional anatomy of multiple areas of visual cortex during sustained spatial attention in humans.

We now understand that visuospatial attention can influence stimulus processing at many stages of cortical visual processing from V1 to IT cortex. Are the effects of attention the same at these different stages of processing, or does attention act at different stages of the visual hierarchy to accomplish different processing goals?

Let's consider some models and evidence that help to answer this question. One prominent model, proposed by Robert Desimone and John Duncan (1995), is known as the biased competition model for selective attention. This model may help answer two questions. First, why are the effects of attention larger when multiple competing stimuli fall within a neuron's receptive field, as in the work of Moran and Desimone that we described earlier? Second, how does attention operate at different levels of the visual hierarchy as neuronal receptive fields change their properties?

In the biased competition model, the idea is that when different stimuli in a visual scene fall within the receptive field of a visual neuron, the bottom-up signals from the two stimuli compete like two snarling dogs to control the neuron's firing. The model suggests that attention can help resolve this competition by favoring one stimulus. Given that the sizes of neuronal receptive fields increase as you go higher in the visual hierarchy, there is a greater chance for competition between different stimuli within a neuron's receptive field and, therefore, a greater need for attention to help resolve the competition.

Sabine Kastner and her colleagues (1998) designed an experiment using fMRI to investigate the biased competition model during spatial attention in humans (Figure 7.22). To do this, they first asked whether, in the absence of focused spatial attention, nearby stimuli could interfere with one another. The answer was yes. They found that when they presented two nearby stimuli simultaneously, the stimuli interfered with each other and the neural response evoked by each stimulus was reduced compared to when one stimulus was presented alone in the sequential condition (Figure 7.22d). If attention was introduced and directed to one stimulus in the display, however, then simultaneous presentation of the competing stimulus no longer interfered (Figure 7.22e). This effect tended to be larger in area V4, where attention appears to have more of an effect than in V1. The attention focused on one stimulus attenuated the influence of the competing stimulus. To return to our analogy, one of the snarling dogs (the competing stimulus) was muzzled.

For a given stimulus, spatial attention appears to operate differently at early (V1) versus later (e.g., V 4) stages of the visual cortex. Why? Perhaps because the neuronal receptive fields differ in size from one visual cortical area to the next. Thus, although smaller stimuli might fall within a receptive field of a single V1 neuron, larger stimuli would not; but these larger stimuli would fall within the larger receptive field of a V4 neuron. In addition, exactly the same stimulus can occupy different spatial scales, depending on its distance from the observer. For example, when you view the flowers in Figure 7.23a from a greater distance (Figure 7.23b), they occupy less of your visual field (compare what you see in the yellow circles, representing a V 4 neuron's receptive field). All of the flowers could, though, fall into the receptive field of a single neuron at a later stage of the visual hierarchy (compare what is within the blue circles, representing the larger receptive field of an IT neuron).

In line with the biased competition model, attention could act to select one flower versus another early in the visual cortical hierarchy (i.e., within V4 when the spatial scale of the flowers was small (Figure 7.23b), but not when the spatial scale was large and there was only a single flower in the receptive field (Figure 7.23a); attention could act only at higher levels of the cortical hierarchy in this case (i.e., above V4). This observation suggests that attention should operate at different stages of vision, depending on the spatial scale of the attended and ignored stimuli. Does it? How would you design a study to answer this question?

Max Hopf, Steven Luck, and colleagues (2006) combined recordings of ERPs, magnetoencephalography (MEG), and fMRI. The simple stimuli they used are shown in Figure 7.24. In each trial, stimulus arrays consisting of four groups of four squares each, appeared in each visual field quadrant (Figure 7.24a). To create a small-spatial-scale target, a single small square was shifted up or down (Figure 7.24b); for a large-scale pattern, one set of four small squares was shifted (Figure 7.24c). Participants were instructed to attend to the arrays of one color (red or green), as instructed beforehand, and to push one of two buttons, depending on whether the displaced squares shifted up or down, regardless of spatial scale. This task required, therefore, that the participants detect the shifted square and focus attention at the appropriate spatial scale to determine the finer detail of the direction of the shift.

The study revealed that attention acted at earlier levels of the visual system (within visual area V 4) for the smaller targets than it did for the larger targets (lateral occipital complex; Figure 7.24d). So, although attention does act at multiple levels of the visual hierarchy, it also optimizes its action to match the spatial scale of the visual task. One may extend this idea to hypothesize that attention to yet other aspects of task-relevant visual stimuli (location, color, motion, form, identity, etc.) would be supported by modulations of processing in visual areas tuned for those stimulus attributes; we will return to this idea later in the chapter, when we discuss feature and object attention.

SUBCORTICAL ATTENTION EFFECTS Could attentional filtering or selection occur even earlier along the visual processing pathways–in the thalamus or in the retina? Unlike the cochlea, the human retina contains no descending neuronal projections that could be used to modulate retinal activity by attention. But massive neuronal projections do extend from the visual cortex (layer V1 neurons) back to the thalamus. These projections synapse on neurons in the perigeniculate nucleus, which is the portion of the thalamic reticular nucleus (TRN) that surrounds the lateral geniculate nucleus (LGN; Figure 7.25).

These neurons maintain complex interconnections with neurons in the thalamic relays and could, in principle, modulate information flow from the thalamus to the cortex. Such a process has been shown to take place in cats during intermodal (visual–auditory) attention (Yingling & Skinner, 1976). The TRN was also implicated in a model to select the visual field location for the current spotlight of attention in perception–an idea proposed by Nobel laureate Francis Crick (1992). Is there support for such a mechanism?

Studies on monkeys in which attention affected the metabolic activity of the LGN neurons provided initial hints that attention might influence LGN processing (Vanduffel et al., 2000). Subsequent studies by Sabine Kastner and her colleagues used high-resolution fMRI to assess whether attention had the same influence in the human LGN (reviewed in Kastner et al., 2006). Researchers presented participants with a bilateral array of flickering checkerboard stimuli (Figure 7.26a), which activated the LGN and multiple visual cortical areas (Figure 7.26b). Participants were cued to attend to either the left or the right half of the array.

The results showed that the amplitude of the activation was greater in the LGN and visual cortex that were contralateral to the attended array (Figure 7.26c, red lines in graphs) compared to the activity in response to the unattended array (black lines). So, highly focused visuospatial attention can modulate activity in the thalamus. Since fMRI studies do not provide timing information, however, it is hard to know what such effects indicate. Do they reflect attentional gating of the afferent LGN neuron's signals heading to V1? Or, instead, do they reflect reafferent feedback to the thalamus from the cortex that is not the incoming afferent volley of information?

Kerry McAlonan and colleagues at the National Eye Institute (2008) recorded from LGN relay neurons and the surrounding TRN neurons of monkeys that had been trained to attend covertly to a target at one location while ignoring other targets. When the monkeys' attention was directed to the location of the stimulus within the LGN neuron's receptive field, the firing rate of the neuron increased (Figure 7.27 a). In addition, however, the firing rate decreased in the surrounding TRN neurons (which, recall, are not relay neurons, but instead are interneurons that receive input from the visual cortex; Figure 7.27b). Why is that? Well, we know from other work that the TRN neurons synapse onto the LGN neurons with inhibitory signals.

We can now explain the entire circuit. Attention involves either activating or inhibiting signal transmisSion from the LGN to visual cortex via the TRN circuitry. Either a descending neural signal from the cortex or a separate signal from subcortical inputs travels to the TRN neurons. These inputs to the TRN can excite the TRN neurons, thereby inhibiting information transmisSion from LGN to visual cortex; alternatively, the inputs can suppress the TRN neurons, thus increasing transmission from LGN to visual cortex. The latter mechanism is consistent with the increased neuronal responses observed for the neurons in LGN and V1 when coding the location of an attended stimulus.

## Reflexive Visuospatial Attention

While we can voluntarily direct our attention to the words on this page or to remembering what we had for breakfast, oftentimes things in the environment attract our attention without our cooperation. This is reflexive attention, and it is activated by stimuli that are salient (conspicuous) in some way. The more salient the stimulus, the more easily our attention is captured: Think of how we respond to rapid movement that we see out of the corner of the eye (eek! a rat!) or the shattering of glass in a restaurant. Heads turn toward the sounds and sights and then wag back a moment or two later, unless the event is behaviorally relevant, in which case we decide to focus our attention on it, using voluntary attention. This head wagging may happen before we can prevent it, because our reflexive attention may lead to overt orienting (heads and eyes turning toward the event) to the sensory stimulus. Even without overt signs of orienting, however, covert attention can be attracted to sensory events. So, we're led to ask, are reflexive and voluntary attention processed in the same way?

To tackle this question, attention researchers have used a variant of the cuing method (Figure 7.15) to demonstrate this phenomenon experimentally (e.g., Jonides, 1981). These studies examine how a task-irrelevant event somewhere in the visual field, like a flash of light, affects the speed of responses to subsequent task-relevant target stimuli that might appear at the same or some other location. This method is referred to as reflexive cuing or exogenous cuing, because attention is controlled by lowlevel features of external stimuli, not by internal voluntary processes. Although the light flash "cues" do not predict the location of subsequent targets, responses are faster to targets that appear in the vicinity of the irrelevant light flash–but only for a short time after the flash, about 50–200 ms. These effects tend to be spatially specific; that is, they influence processing in and around the location of the reflexive cue only. Therefore, they can also be described by the spotlight metaphor introduced earlier in this chapter. In this case, however, the spotlight is reflexively attracted to a location and is short-lived.

Interestingly, when more than about 300 ms pass between the task-irrelevant light flash and the target, the pattern of effects on reaction time is reversed: Participants respond more slowly to stimuli that appear in the vicinity of where the flash had been. This phenomenon is called the inhibitory aftereffect or, more commonly, inhibition of return (IOR) –that is, inhibition of the return of attention to that location.

Consider the advantages of this kind of system. If sensory events in the environment caused reflexive orienting that lasted for many seconds, people would be continually distracted by things happening around them and would be unable to attend to a goal. Our ancestors might never have made it to reproductive age and thus, we wouldn't be here reading this book. While watching for a lion or looking for food, they might have been distracted and entranced by a bird's song, and ... whoops, missed the lion! Or whoops, no meal again! In today's world, imagine the consequences if a driver's attention became reflexively focused on a distraction off to the side of the road and then remained focused on that event for more than an instant.

Our automatic orienting system has built-in mechanisms to prevent reflexively directed attention from becoming stuck at a location for more than a couple hundred milliseconds. The reflexive capturing of attention subsides, and the likelihood that our attention will be drawn back to that location is reduced slightly. If the event is important and salient, however, we can rapidly invoke our voluntary mechanisms to sustain attention longer, thereby overriding the inhibition of return. Thus, the nervous system has evolved clever, complementary mechanisms of voluntary and reflexive attention so that we can function in a cluttered, rapidly changing sensory world.

Responses to endogenous and exogenous cues result in attention shifts that enhance the processing of attended sensory stimuli and decrease the processing of unattended stimuli. In the case of reflexive attention, the cuing effect is quick and short-lived, and processing of stimuli in the neighborhood of the cue is enhanced. With voluntary attention, however, the cuing effect is slower and more sustained. Do these differences in processing represent different neural mechanisms?

We have learned that voluntarily focusing attention at a location in response to visual cues will enhance the visual responses to stimuli occurring at that location. Do these same neural changes occur when our attention is reflexively attracted to a location in the visual field by a sensory event? Joseph Hopfinger and George Mangun (1998, 2001) answered yes to this question. They recorded ERPs in response to target stimuli in a reflexive cuing task like the one described earlier (Figure 7.28a). They found that the early occipital Pl wave is larger for targets that quickly follow a sensory cue at the same location, versus trials in which the sensory cue and target occur at different locations. As the time after cuing grows longer, however, this effect reverses and the Pl response diminishes–and may even be inhibited–just as in measurements of reaction time (Figure 7.28b).

These data indicate that both reflexive (stimulus-driven) and voluntary (goal-directed) shifts in spatial attention induce similar physiological modulations in early visual processing. Presumably, the neural networks implementing these attentional modulations of sensory analysis are different, reflecting the differing ways in which attentional control is triggered for the two forms of attention.

## Visual Search

In everyday perception, voluntary attention (driven by our goals) and reflexive attention (driven by stimuli in the world) interact in a push–pull fashion, struggling to control the focus of our attention. For example, we frequently search about for a specific item in a cluttered scene. Perhaps we watch for a friend coming out of class or for our suitcase on the baggage claim carousel of a busy airport. If the suitcase is red and covered with flowered stickers, the search is quite easy. If the suitcase is a medium-sized black bag with rollers, however, the task can be quite challenging.

As you cast your gaze around for that friend or suitcase, you don't keep going back to places that you have just scanned. Instead, you are biased, moving your eyes to new objects in new locations. The last time you stood in baggage claim, you probably didn't wonder what role attentional processes play in this visual search process. How are voluntary and reflexive spatial attention mechanisms related to visual search?

The great experimental psychologist Anne Treisman and her colleagues were interested in the mechanisms of visual (attentional) search. In one set of experiments, they observed that a target item can be located more quickly among a field of distractor stimuli if it can be identified by a single stimulus feature, such as color (e.g., a red suitcase among black ones; or, in the lab, a red O among green X s and O's presented on a computer screen). It doesn't matter how many distractors appear in the array in this situation.

We can demonstrate this relation by plotting participants' reaction times as a function of the number of distractor items in the display (search function), as shown in Figure 7.29. When the target can be identified by a single feature, such as the red O in Figure 7.29a, the resulting search function is rather flat (Figure 7.29c, blue line). This phenomenon is called pop-out because the red O literally appears to pop out of the array of green letters on the basis of its color alone.

If the target shares features with the distractors, however, so that it cannot be distinguished by a single feature (e.g., a red O among green Xs and O's and red X s, as in Figure 7.29b, or a red suitcase among black suitcases and differently shaped red and black backpacks), then the time it takes to determine whether the target is present or absent in the array increases with the number of distractors in the array. The resulting search function is a sloped line (Figure 7.29c, red line). This type of search is known as a conjunction search because the target is defined by the conjunction of two or more features (e.g., the color red and the letter's identity as an O, or the color and shape of the travel totes).

To explain why conjunction targets take longer to find, Treisman and Gelade (1980) proposed that while elementary stimulus features such as color, motion, shape, and spatial frequency can be analyzed preattentively (without attention) and in parallel within multiple specialized feature maps (located within specialized visual cortical areas), something else is required to bind the disparate feature signals together. The researchers suggested that spatial location was the key, and that therefore spatial attention was the mechanism that "glued" the features together.

According to their formulation, spatial attention must be directed to relevant stimuli in order to integrate the features into the perceived object, and it must be deployed in a sequential (serial) manner for each item in the array. This condition is necessary to link the information (in this case, color and letter identity, or travel tote color and shape) in the different feature maps so that the target can be analyzed and identified. This concept is called the feature integration theory of attention, and understanding the neural bases of these mechanisms remains of central importance in cognitive neuroscience.

If Treisman was correct, then we might hypothesize that Treisman's "glue" is the same as Posner's "spotlight of attention." Steven Luck and his colleagues (1993) tested this idea using ERP methods. They reasoned that if Posner's spatial spotlight of attention served as the glue for feature integration, then, during visual search for a conjunction target, they should be able to reveal the spatial spotlight using ERPs, just as was done in studies of voluntary and reflexive spatial attention (see Figure 7.17). They presented participants with stimulus arrays that contained a field of upright and inverted blue, green, and red t shapes. In each trial, the target they were to detect was a t of a particular color (blue or green, which was assigned before each block of trials; Figure 7.30a).

In order to measure whether spatial attention was allocated selectively to locations in the arrays, the researchers used an ERP probe method: At brief time intervals after the search array was presented, a solitary task-irrelevant probe stimulus, which had no features in common with the targets or distractors, was flashed either at the location of the designated target item (e.g., the green t) or at the location of a distractor item on the opposite side of the array (e.g., the blue t). The probe stimulus, which itself elicited an ERP, was the white outline of a square, which appeared around either the blue t or the green t, but never around a red t. The neural responses to the probes could then be used to reveal whether the typical spatial attention effects in visual cortical processing seen with ERPs in cuing tasks were present during visual conjunction search. The researchers found that indeed they were.

The probe elicited larger early visual responses (Pl waves) at the location of the designated conjunction target (where attention was focused), as compared to regions where only distractors were present. This study showed that conjunction search affects the Pl wave in much the same way that cued spatial attention does (compare Figure 7.30b with Figure 7.17b), supporting the idea that a kind of spotlight of spatial attention is employed during visual search, and revealing the neural basis of that effect: modulations of early cortical visual processing.

## Feature Attention

As our own experience tells us, we have learned that selectively attending to spatial locations, either voluntarily or reflexively, leads to changes in our ability to detect and respond to stimuli in the sensory world, and our improved detection is promoted by changes in neural activity in the visual cortex. Yet the question remains, Does spatial attention automatically move freely from item to item until the target is located, or does visual information in the array help guide the movements of spatial attention among the array items? As Robert Louis Stevenson pointed out, the world is full of objects of interest; however, some are more interesting than others. For instance, when you gaze across the expanse of Monument Valley (Figure 7.31), your attention is drawn to the buttes and mesas, not to a random bush. Why?

Objects are defined by their collection of elementary features, as we discussed in Chapters 5 and 6. Does selectively attending to a specific stimulus feature (e.g., motion, color, shape) or to a set of object properties (e.g., a face versus a house) influence information processing? For instance, if we are cued to expect that an upcoming stimulus is moving, are we better able to discriminate the target stimulus if it is indeed moving rather than unexpectedly stationary? If your friend says she will pick you up at the airport and will drive around the terminals until you spot her, will it take you longer to find her if she is parked at the curb instead? And, of course, we still want to know how feature and spatial attention interact, given that the world is full of features and objects located in specific locations.

Marissa Carrasco and her colleagues at New York University performed a set of experiments to address these questions. They compared spatial attention and feature attention in a voluntary cuing paradigm (Liu et al., 2007). The dependent measure of attention was detection accuracy. In one condition (using spatial attention), arrow cues indicated the location where attention should be directed. In the other condition (the feature attention condition), arrows indicated the direction of motion of the upcoming target (Figure 7.32a).

The researchers found that prior knowledge from the cue produced the typical voluntary cuing effect for spatial attention: Participants were more accurate at detecting the presence of the target (a change in the velocity of moving dots) at the cued location compared to when the cue (a double-headed arrow) did not signal one location over another (Figure 7.32b, red line). In a similar vein, during the feature attention condition, cuing the direction of motion of the target also enhanced accuracy independently of whether it appeared in the leftor rightvisual-field array (Figure 7.32b, blue line). Thus, precuing attention to a visual feature (in this case, motion direction) can improve performance. This finding tells us that attention can be directed in advance both to spatial locations and to nonspatial features of the target stimuli.

Let's now ferret out the neural bases of selective attention to features and objects, and contrast these mechanisms with those of spatial attention. In the early 1980s, Thomas Münte, a German neurologist working with Steven Hillyard in San Diego, developed a clever experimental paradigm to investigate spatial and feature attention mechanisms (Hillyard & Münte, 1984).

Using ERPs, they isolated the brain responses that are related to selectively attending stimulus color from those related to attending stimulus location. Rather than cuing participants to different stimulus features, they presented participants with blocks of many trials in which small red and blue vertical rectangles, some tall and some short, were flashed in a random sequence in the left and right visual fields. Each block of trials lasted a minute or so. Participants fixated on the central crosshairs on the screen while covertly attending to one color–red or blue–at the attended location, left or right. For example, participants were told, "For the next minute, attend and push the button to the shorter red bars on the right only." Thus, they had to ignore the other color at the attended location, as well as both colors at the unattended location. There were four different attention conditions, and the investigators could compare the ERPs generated under each one.

In this ingenious setup, the comparisons revealed independent processing for spatial attention and feature attention. For example, for a left, red stimulus, spatial attention (attend left versus attend right) could be experimentally uncoupled from feature attention (attend red versus attend blue). The brain responses for each of these conditions are shown in Figure 7.33. In Figure 7.33a, the ERPs show the typical spatial attention effects seen in Figure 7.17 (solid versus dashed ERP traces). Figure 7.33b shows the feature attention ERPs. Note the very different patterns that spatial and feature (in this case, color) attention produced in the ERPs. These are especially obvious in the ERP attention difference waves (Figure 7.33c and d), where the waveform elicited by the stimulus when ignored is subtracted from the waveform elicited by the same stimulus when attended. The early Pl attention effect that indexes spatial attention (Figure 7.33c) is absent for feature attention (Figure 7.33d), which shows only longer latency changes in the difference waveform. Also of interest from this work is the fact that the effects of attention to color stimuli were largely absent at the unattended location (Figure 7.33d). This research indicates that both spatial attention and feature attention can produce selective processing of visual stimuli, and that their mechanisms differ.

Where do these feature attention effects take place in the brain? Well, it depends. Maurizio Corbetta and his colleagues at Washington University investigated the neural systems involved in feature discrimination under two different conditions: divided attention and selective attention (Corbetta et al., 1991). In this groundbreaking neuroimaging study of selective attention, the researchers used PET imaging to identify changes that occur in extrastriate cortex and elsewhere when people selectively attend to a single stimulus feature, such as color, shape, or motion, versus when their attention is divided among all three features.

Participants underwent PET scans while being shown pairs of visual displays containing arrays of stimulus elements. The first display of each trial was a reference stimulus, such as a red square; the second was a test stimulus, perhaps a green circle. During the selective-attention condition, participants were instructed to compare the two arrays to determine whether a change had occurred to a pre-specified stimulus dimension (color, shape, or motion). During the divided-attention condition, participants were instructed to detect a change in any of the three stimulus dimensions.

This experiment design permitted the investigators to contrast brain activity under conditions in which the participants selectively attended a particular stimulus dimension (e.g., only color) with the condition in which they divided their attention among all stimulus dimensions. As you might expect, behavioral sensitivity for discriminating slight changes in a stimulus was higher when participants were judging only one feature (selective attention) rather than multiple features (divided attention).

Compared to divided attention, selective attention to one feature activated distinct, largely nonoverlapping regions of extrastriate cortex (Figure 7.34). Extrastriate cortical regions specialized for the perceptual processing of color, shape, or motion were modulated only during visual attention to the corresponding stimulus features. These findings provide additional support for the idea that selective attention, in modality-specific cortical areas, alters the perceptual processing of inputs before the completion of feature analysis.

Subsequent fMRI studies have identified specialized areas of human visual cortex that process features such as stimulus motion or color. Corresponding areas had been found previously in monkey visual cortex. These specialized feature analysis regions are modulated by selective visual attention, as suggested by the earlier work of Corbetta and colleagues.

When do these various attention effects occur during processing? To address this question, one study combined MEG and fMRI to provide temporal and spatial information (Schoenfeld et al., 2007). Participants were cued to attend selectively to either changes in color or changes in motion in an upcoming display (Figure 7.35a). The stimulus sequence randomly presented motion and color changes, permitting the measurement of brain activity in response to changes in either feature as a function of attention to motion or color.

By using fMRI to localize brain regions sensitive to selective attention to color or motion, the investigators found (as expected) that attending to motion modulated activity in the visual cortical motion-processing area MT/V5 (in the dorsal stream; Figure 7.35b). Similarly, attending to color led to modulations in ventral visual cortex area V4v (in the posterior fusiform gyrus; Figure 7.35c and d). Importantly, the team's related MEG recordings demonstrated that attention-related activity in these areas appeared with a latency of 100 ms or less after onset of the change in the stimulus–much sooner than previous studies had reported.

Thus, feature-based selective attention acts at relatively early stages of visual cortical processing with relatively short latencies after stimulus onset. Spatial attention, however, still beats the clock and has an earlier effect. We see that the effects of feature attention occur with longer latencies (100 ms versus 70 ms after stimulus onset) and at later stages of the visual hierarchy (extrastriate cortex rather than striate cortex or the subcortical visual relays in the thalamus) than does spatial attention. This difference does not indicate that spatial attention always drives our attention, because if one does not know where a stimulus will appear but does know which features are relevant, as during visual search, feature attention may provide the first signal that then triggers spatial attention to focus on a location.

## Object Attention

Now that we've described the effects of spatial-based attention and feature-based attention in visual cortex, let's turn to another question: Can attention also act on higher-order stimulus representations–namely, objects? When searching for a friend in a crowd, we don't merely search where we think our friend will be, especially if we haven't agreed on a place to meet. We also don't search for our friend only by hair color (unless it is highly salient, like fluorescent pink). Rather, we look for the conjunction of features that defines the person. For lack of a better word, we can refer to these qualities as object properties–elementary stimulus features that, when combined in a particular way, yield an identifiable object or person. Behavioral work has demonstrated evidence for object-based attention mechanisms.

In a seminal study, John Duncan (1984) contrasted attention to location (spatial attention) with attention to objects (object-based attention). Holding spatial distance constant, he discovered that two perceptual judgments concerning the same object could be made simultaneously without loss of accuracy, whereas the same two judgments about different objects could not. For instance, in a split second you can process that a dog is big and brown, but when two dogs are present, processing that one is big and the other is brown takes longer.

This processing limitation in attending to two objects implicates an object-based attention system in addition to a space-based system. In line with this view, the costs (slowing) that the spatial cues of attention confer on behavioral reaction time are greater than the benefits (speeding) when two objects are being attended as compared to one object (Egly et al., 1994). This result suggests that the spread of attention is facilitated within the confines of an object, or that there is an additional cost to moving attention between objects, or both.

Notger Mueller and Andreas Kleinschmidt (2003) designed an fMRI study to determine what effect objects had on spatial attention. They wondered whether attending to an object had any impact on processing in the early visual processing areas, and if so, what sort of impact? They cued participants on a trial-by-trial basis to expect a target at one location in the visual field (e.g., upper-left quadrant) and then presented targets there on most trials (valid trials). In a minority of trials, they presented the targets to uncued locations (invalid trials). Following the design of Egly and colleagues (1994), Mueller and Kleinschmidt included objects on the screen so that the uncued target could fall either within the same object that was cued (but at another location in that object), or at another location that was not within the bounds of the object. Figure 7.36a illustrates their design.

The displayed objects were wrench-like figures, and these figures could be oriented horizontally on the screen or vertically. For example, when the wrenches were oriented horizontally and the upper-left-quadrant location was cued, the upper-right-quadrant location would be spatially uncued (unattended) but be within the same object. When the wrenches were vertically oriented, however, that location would be spatially uncued and within a different object.

Mueller and Kleinschmidt replicated the behavioral reaction time effects of Egly and colleagues (Figure 7.36b). What's more, they found that in visual cortical areas V1 through V4, increased activity occurred in uncued locations that were located on the same object (the wrench) as the cued location, compared to when the uncued location was not on the cued object (Figure 7.36c and d).

This result is evidence that the presence of objects influences the way spatial attention is allocated in space: In essence, attention spreads within the object, thereby leading to some activity for uncued locations on the object as well. An effect of spatial attention also remains, because, within the object, the cued location still shows greater activity than uncued locations show. Thus, object representations can modulate spatial attention. Can attention to objects also operate independently of spatial attention?

An ingenious fMRI study addressing this question was conducted by Kathleen O'Craven, Paul Downing, and Nancy Kanwisher (O'Craven et al., 1999). They knew that in humans, images of faces activated the fusiform face area (FFA; see Chapter 6) more than did other objects, and that another area in the posterior parahippocampal cortex– the so-called parahippocampal place area (PPA)–was more activated by images of landscapes, scenery, and structures such as houses. The researchers reasoned that selective attention to faces or houses should differentially modulate activity in the FFA and P PA, with attention to faces boosting responses in the FFA but not the PPA, and vice versa. Moreover, attention to motion should affect motion-sensitive brain regions (MT/MST; also known as MT/V5, or MT+) but not the FFA or PPA.

O'Craven's group tested this hypothesis directly using fMRI in a face–house–motion selective-attention paradigm, where superimposed faces and houses (Figure 7.37a) that were either moving or static were presented in different attention conditions: attend faces, attend houses, or attend motion. The first experiment used a block design: In separate blocks, participants were instructed to selectively attend the faces, the houses, or the motion direction. Their task was to detect when the same attended item appeared twice in a row (e.g., same person, same house, or same direction of motion). Figure 7.37b shows the resulting pattern of brain activity.

As predicted, attention to faces produced the largest signals in the FFA, attention to houses produced the largest signals in the PPA, and attention to motion caused the greatest activity in MT/MST. This pattern of results supports the idea that attention can act on object representations in a highly selective fashion, independent of spatial attention; in other words, because the two objects–faces and houses–were overlapping in space, it was not possible simply to use spatial attention to focus on one object and not the other.

In the second experiment, the researchers tested whether task-irrelevant attributes of an attended object would be selected along with the task-relevant attribute, which is a central claim of object-based theories. Using an event-related design, they presented randomly intermixed trials of either moving faces superimposed on stationary houses, or moving houses superimposed on stationary faces. Participants were told either to attend selectively to the direction of motion (regardless of whether the face or house was moving) and detect when two consecutive presentations of the stimuli contained the same direction of motion, or to attend to the position of the static image on the screen (regardless of whether it was the face or the house that was static) and detect when two consecutive presentations of the stimuli had the same position on the screen.

As Figure 7.37 c shows, even though the taskrelevant stimulus attributes were motion and position, independent of the task-irrelevant face–house distinction, attention to the shared irrelevant attribute (face) produced greater activity in the FFA than attention to the task-relevant attributes (motion or position) shared with the house stimulus. The inverse pattern was seen in the P PA, where activity was greater when the attended attributes were shared with the house stimulus.

These results demonstrate how attention acts on object representations and show that objects, being composed of many features, show interesting properties. For example, attention facilitates processing of all the features of the attended object, and attending one feature can facilitate the object representation in object-specific regions such as the FFA. Importantly, these findings show that even when spatial attention is not involved, object representations can be the level of perceptual analysis affected by goal-directed attentional control.

SPIKES, SYNCHRONY, AND ATTENTION We now know that when attention is focused on a stimulus, neurons in the visual system that code that stimulus increase their postsynaptic responses and their firing rates. How does this happen in a selective fashion so that attended information is routed appropriately to influence subsequent stages of processing? Although we remain uncertain about the precise mechanisms, various hypotheses have been proposed, and some interesting models are being tested. One such model suggests that at different stages of visual analysis (e.g., V1 and V4), neurons that code the receptive-field location of an attended stimulus show increased synchrony in their activity.

Conrado Bosman, Pascal Fries, and their colleagues (Bosman et al., 2012) used cortical surface grids of more than 250 electrodes in the monkey to test this model. They presented monkeys with two drifting gratings separated in visual space, and they trained the monkeys to keep their eyes fixed on a central crosshair but covertly attend one drifting grating at a time to detect when the shape of the gratings changed slightly. Given the retinotopic organization and small receptive-field size of V1 (about 10 of visual angle), stimuli separated by several degrees activate different populations of neurons in V1. In higher-order visual areas like V 4, which have much larger receptive fields (several degrees of visual angle), however, the same stimuli fall within the receptive field of the same V4 neuron (Figure 7.38a).

The researchers hypothesized that if spatial attention can alter the flow of information from early stages of the visual hierarchy (V1) to later stages (V4) in a spatially specific manner, then this effect might promote selective synchronization of local field potentials (LFPs) between these early and later stages of visual processing (Figure 7.38b). That is precisely what they observed. They measured the cortical surface LFPs oscillating within the gamma-band frequencies of 60–80 Hz and found that coherence increased with spatial attention between the site in V1 that coded the attended stimulus location (e.g., location Vla in the figure) and the V4 site that coded the stimulus location.

So, if the monkey attended location Vla, it showed increased synchronization in gamma-band LFPs with V4 (Figure 7.38c, red). At the same time, however, the coherence remained low between the other V1 site that coded the ignored location (e.g., location Vla in the figure) and V4 (Figure 7.38c, blue). Interestingly enough, though, when the animal was cued to switch attention to the other stimulus location (i.e., V 1b in the figure), then the V1–V4 coherence went up for that V1 site and V4, and coherence at the first location dropped (Figure 7.38d, blue versus red).

These studies suggest that attention alters the effective connectivity between neurons by altering the pattern of rhythmic synchronization between areas, and that different brain regions can communicate through the coupling of their oscillation patterns. Bosman and colleagues believe that such communication occurs when the attended signals passing from V1 to V4 induce the gamma neuronal synchronization with V4.

## TAKE-HOME MESSAGES 

- Spatial attention influences the processing of visual inputs: Attended stimuli produce greater neural responses than do ignored stimuli, and this process takes place in multiple visual cortical areas. Reflexive attention is automatic and is activated by stimuli that are conspicuous in some way.
- Reflexive attention also results in changes in early sensory processing, although only transiently.
- A hallmark of reflexive attention is inhibition of return, the phenomenon in which the recently reflexively attended location becomes inhibited over time such that responses to stimuli occurring there are slowed
- Extrastriate cortical regions specialized for the perceptual processing of color, shape, or motion can be modulated during visual attention to the individual stimulus features. 
- Selective attention can be directed at spatial locations, at object features, or at an entire object.
- Attention increases coherence of neuronal oscillations between visual areas.

---

# 7.5 Attentional Control Networks

Thus far, we have been considering the influence of attention on sensory processing; in other words, we have been looking at the sites of attention's influence. This is only part of the attention story. For the rest of the chapter we turn to how the focus of attention is controlled, which will help us understand how neurological damage affected attention in the patients we described earlier in the chapter.

As we now know, attention can be either goal directed (top-down) or stimulus directed (bottom-up). Right now, you are using top-down, goal-directed attention to focus on this book. How does this work? According to the general model that has been with us for decades, top-down neuronal projections from attentional control systems (with inputs about goals, information learned by experience, reward mechanisms, etc.) contact neurons in sensory-specific cortical areas to alter their excitability. As a result, the response in the sensory areas to a stimulus may be enhanced if the stimulus is given high priority, or attenuated if it is irrelevant to the current goal. We observe these effects of attention as changes in behavior and neural activity.

Much converging evidence indicates that selective attention may influence cortical excitability in the visual cortex (and other sensory systems) through a control network that includes at least the posterior parietal cortex, the posterior superior temporal cortex, the dorsolateral and superior prefrontal cortex, the medial frontal cortex (such as anterior cingulate cortex), and perhaps the pulvinar nucleus of the thalamus (Figure 7.39). More generally, though, attentional control systems are involved in modulating thoughts and actions, as well as sensory processes.

Studies of patients with either unilateral neglect or Bálint's syndrome have provided us clues about the control of attention. As noted earlier in the chapter, bilateral lesions to portions of the posterior parietal and occipital cortex result in Bálint's syndrome, and unilateral lesions of the parietal, temporal, and frontal cortex, especially in the right hemisphere, are implicated in neglect. Neglect may also result from damage to subcortical structures like the superior colliculus and parts of the thalamus. Some neurologists, including M.-Marsel Mesulam (1981), have suggested that the disorder of neglect was the result of damage to the brain's attention network rather than to any specific brain area (e.g., parietal cortex). Which structures constitute the brain's attentional control network? Does a single network control attention, or are multiple networks involved?

Current models of attentional control suggest that two separate cortical systems are at play in supporting different attentional operations during selective attention: a dorsal attention network, concerned primarily with voluntary attention based on spatial location, features, and object properties; and a ventral attention network, concerned with stimulus novelty and salience (Corbetta & Shulman, 2002). It appears that the two control systems interact and cooperate to produce normal behavior, and that these interactions are disrupted in patients with neglect. These models are based on behavioral studies in healthy persons or in patients with brain lesions, as well as on the results of neuroimaging and electrophysiology experiments.

## The Dorsal Attention Network

Joseph Hopfinger and his colleagues (2000) and Maurizio Corbetta and his coworkers (2000) both employed eventrelated fMRI to study attentional control. We reviewed some of the findings from Hopfinger's study earlier in this chapter, focusing on how spatial attention involves selective processing in visual cortex (the site of attention). Now we return to this research to see what these investigators learned about the brain regions that control attention. Later we will discuss the work of Corbetta and colleagues to complete the story.

FINDING THE SOURCES OF ATTENTIONAL CONTROL OVER SPATIAL ATTENTION Recall that Hopfinger and his coworkers used a modified spatial cuing paradigm like the one shown in Figure 7.15. The participants were presented a cue and were required on some trials to orient attention to one half of the visual field and ignore the other. Then, after a delay, two stimuli were presented simultaneously, one in each visual hemifield, and the participant was to discriminate target features of the stimulus at the pre-cued location and make a response. Earlier we focused on the brain activity triggered by the appearance of the target stimuli, but to investigate attentional control these researchers looked earlier in the trial at the brain activity after the cue but before the targets appeared. Such activity can be ascribed to goal-directed attentional control.

What did the researchers find? When the participant attended and responded to the stimulus, a network of dorsal frontal and parietal cortical regions showed increased activity. These regions together are now called the dorsal attention network. None of the regions in this network were primarily involved in sensory processing of the visual features of the cue, which took place in the visual cortex. We now understand that this dorsal frontoparietal network reflects the sources of attentional signals in the goal-directed control of attention.

Why did the researchers conclude that these regions are involved in attentional control? First, the identified brain regions were found to be active only when the participants were instructed (cued) to covertly attend either right or left locations. Second, when the targets appeared after the cue, a different pattern of activity was observed. Third, when participants only passively viewed the presented cues—and didn't attend to them or act on them— then the frontoparietal brain regions that were active in the former condition were not activated during passive viewing, even though the visual cortex was engaged in processing the visual features of the passively viewed cues.

The key cortical nodes involved in the dorsal attention network are the frontal eye fields (FEFs, located at the junction of the precentral and superior frontal sulcus in each hemisphere) and the supplementary eye fields (SEFs) in the frontal cortex; the intraparietal sulcus (IPS), superior parietal lobule (SPL), and precuneus (PC) in the posterior parietal lobe; and related regions (Figure 7.40). From studies like Hopfinger's, we know that the dorsal attention network is active when voluntary attention is engaged. How does this network function to modulate sensory processing?

LINKING THE CONTROL NETWORK FOR SPATIAL ATTENTION TO ATTENTIONAL CHANGES First let's look at the evidence that activity in the dorsal attention network is actually linked to attention-related changes in sensory processing. In Hopfinger's study, after the cue was presented but before the target displays appeared, activations were observed not only in the dorsal attention network, but also in visual cortical regions that would later process the incoming target (Figure 7.41).

These activations in visual cortex were spatially specific—dependent on the locus of spatial attention within the visual field. What caused the visual cortex to be selectively activated even before any stimuli were presented? lhe idea is that these activations reflect a sort of attentional ' 'priming" of the sensory cortex for information coming from a particular location in the visual field. This priming is thought to enable the enhanced neural responses to attended-location stimuli that we detailed earlier in humans and monkeys (see, for example, Figures 7.17, 7.18, and 7.20).

The priming of visual cortex described here from fMRI resembles what has been observed under similar circumstances in neurophysiological studies in monkeys (Luck et al., 1997). Such priming is possible if neurons in the dorsal attention network send signals either directly or indirectly to the visual cortex, producing selective changes in visual processing in those visual neurons (e.g., biasing inputs in favor of one location versus another). Do any data support this biasing effect on the visual cortex?

FRONTAL CORTEX AND ATTENTIONAL CONTROL Indirect evidence comes from patients with prefrontal cortical lesions. Neurologist Robert T. Knight and his colleagues (Barce16 et al., 2000) found that patients with frontal cortex damage due to stroke had "decreased" visually evoked responses in ERP recordings over visual cortex. This evidence suggests that the frontal cortex (source) has a modulatory influence on the visual cortex (site).

More direct evidence comes from intracranial studies in monkeys. As mentioned earlier, a key component of the frontoparietal attention network is the frontal eye fields. The FEFs are located bilaterally in a region around the intersection of the middle frontal gyrus with the precentral gyrus in the dorsal—lateral—posterior portions of the prefrontal cortex (Figure 7.40). They coordinate eye movement and gaze shifts, which are important for orienting and attention. Stimulation of FEF neurons produces topographically mapped saccadic eye movements.

Tirin Moore and his colleagues at Stanford University (Moore & Fallah, 2001) investigated reports suggesting that brain mechanisms for planning eye movements and directing visuospatial attention overlapped. To confirm whether such overlapping existed, they looked at whether altering oculomotor signals within the brain by stimulating them with electrodes would affect spatial attention. Using intracortical electrical stimulation and recording techniques in monkeys, they stimulated FEF neurons with very low currents that were too weak to evoke saccadic eye movements, but strong enough to bias the selection of targets for eye movements.

Was there any effect on attention? Yes! While the monkey was performing a spatial attention task, the weak FEF stimulations resulted in enhanced performance in the attention task, and these effects were spatially specific. Attention was enhanced to attended targets only if the targets were at the right spot—the specific location in space where the saccadic eye movements would have been directed, if the stimulation to the FEF had been strong enough to generate them.

This finding led the researchers to hypothesize that if FEF microstimulation initiates both saccade preparation and improved attention performance, then it also might induce a spatial-attention-like modulation of the visual cortex (Moore & Armstrong, 2003). To test this hypothesis, they placed a stimulating electrode in FEF that could deliver very weak electrical stimulation. This time, they also recorded from V4 neurons whose receptive fields were located in the visual field where stimulation of the FEF would direct a saccade (Figure 7.42a).

First they presented a stimulus to the receptive field of the V4 neuron. The stimulus was one of two types: either preferred or non-preferred for that particular neuron. The neuron's elicited response was always weaker in the case of the non-preferred stimulus. Then stimulation was applied to the FEF site 200 to 500 ms after the appearance of the visual stimulus. This delay enabled the investigators to examine the effects of FEF stimulation on the activity in V4 that was evoked by the visual stimulus, as opposed to any changes in V4 activity that might have been the direct result of FEF stimulation alone.

The FEF stimulation could have had one of three results: It could have amplified the V4 activity, interfered with it, or had no effect on it. What happened? While the monkey was fixating on a central point on the screen, weak stimulation of the FEF-enhanced stimulus evoked V4 activity (i.e., it increased the number of spikes per second) for the preferred over the non-preferred stimulus (Figure 7.42b). If the V4 neuron was not activated by the visual stimulus, then stimulation of the FEF did not affect the activity of the V4 cell. This result mimics the ones observed when monkeys attend and ignore stimuli in V4 (Figure 7.18). FEF signals appear to participate in goal-directed attentional control over V4 activity.

The fact that microstimulation of the FEF in monkeys modulated the neuronal responses in the posterior visual fields is evidence that goal-directed signals from the frontal cortex cause modulations of neuronal activity. What is the nature of these signals? Are they task specific? For instance, if your task is to identify a face, will goal-directed signals alert only the fusiform face area? Or are signals more broadly transmitted, such that the motion area would also be alerted? Yosuke Morishima and his colleagues (2009) set their sights on answering these questions.

They designed an attention task in which human participants were cued on a trial-by-trial basis to perform a visual discrimination task for either motion direction or face gender. The cue was followed by either a short interval of 150 ms or a long interval of 1,500 ms before the stimulus was presented. The stimulus was a vertical grating that moved to the right or the left, superimposed on an image of a male or female face. In half of the trials, 134 ms after the cue the FEF was stimulated using transcranial magnetic stimulation (T MS).

Morishima and coworkers used TMS at levels low enough not to affect task performance. The goal here was not to modify attentional-related processing in FEF neurons; instead, TMS was used simply to evoke a signal from the FEF to regions of the visual cortex that were functionally interconnected with it. These evoked changes in visual cortex activity with TMS could then be measured by recording ERPs generated by the visual cortex activity, either in the human motion-processing area MT/ V 5, or in the face-processing area, the FFA.

The results revealed that when participants were cued to discriminate the motion stimulus, the TMSinduced activity in MT/ V 5 was increased, but when they were cued to discriminate the gender of the face, the same TMS was found to induce increased activity in the FFA (Figure 7.43). Thus, impulses from the FEF actually coded information about the task that was to be performed, indicating that the dorsal system is involved in generating task-specific, goal-directed attentional control signals.

This study neatly demonstrates that the FEF, a component of the dorsal attention network, has an influence on visual cortex. This goal-directed influence is task specific, such that the functional connectivity between FEF and specific visual areas is increased as a function of the specific state of attention (e.g., attend face versus attend motion). Earlier we described the work of Bosman and Fries, and how it revealed the mechanism by which neurons at different levels of the visual hierarchy (i.e., V1 to V 4) could interact via neuronal synchrony of oscillatory signals. Does this inter-area synchrony in visual cortex arise from intrinsic properties, or is it biased by top-down control signals? Further, could oscillatory synchrony also support the interactions between frontal cortex and visual cortex? Let's consider some evidence.

Georgia Gregoriou, Robert Desimone, and their colleagues (2009) investigated whether the FEF is a source of enhanced neuronal synchrony effects in area V4 during spatial attention, by simultaneously recording spikes and local field potentials from FEF and V4 in two monkeys while the monkeys performed a covert attention task. The researchers found that when the monkeys attended to a stimulus that was in the receptive field of both the FEF and V4, oscillatory coupling between the two areas was enhanced, especially at gamma frequencies. The timing of the recorded signals indicated that the coupling was initiated by FEF. The researchers speculated that coupling at gamma frequencies may optimize the postsynaptic impact of spikes from one visual cortical area on the other, the end result being that communication in the visual cortex is improved with attention.

Another study from the same lab investigated the neural responses when we attend object features that cannot be spatially separated (Baldauf and Desimone, 2014). In this study the researchers used the superimposed face—house stimuli as in Figure 7.37a. The stimuli phased in and out, and the volunteers were cued to attend to either the face or the house and detect an occasional target. Using MEG and fMRI, the researchers found that a particular region in the prefrontal cortex (PFC)—the inferior frontal junction (IFJ)—played an important role in the top-down control of feature-based attention. As expected, attention to faces enhanced the sensory responses in the FFA, and attention to houses enhanced sensory responses in the PPA.

Of interest to us here, depending on whether a face or a house was attended, the increases In sensory responses in the FFA or PPA were accompanied by induced gamma synchrony between the IFJ and either the FFA or the PPA, and the IFJ appeared to be the driver of the synchrony. The researchers note that these two studies demonstrate striking parallels in neural mechanisms: In both, the PFC seems to be the source of top-down biasing signals, with the FEF providing signals for spatial attention and the IFJ providing signals for object or feature attention. (Could periodic stimulation of the frontal cortex increase our attention and help us concentrate? See Box 7.1.)

THE PARIETAL CORTEX AND CONTROL OF ATTENTION The areas along the intraparietal sulcus (IPS) and the superior parietal lobule (SPL) in the posterior parietal lobe are the other major cortical regions that belong to the dorsal attention network (Figure 7.40). The parietal lobe has extensive connections with subcortical areas like the pulvinar and the frontal cortex, as well as other parts of the visual pathways. It contains multiple representations of space. What is the role of the parietal cortex in attention? Numerous physiological studies in monkeys have found that attentional shifts are correlated with significant changes in the activity of parietal neurons. Whenever attention is directed to a stimulus, the firing rates of primate parietal neurons increase when using the stimulus as a target for a saccade or a reaching movement (Mountcastle, 1976) and when covertly discriminating its features (Wurtz et al., 1982). When a monkey is merely waiting for the next trial in a sequence of trials, however, the parietal neurons do not usually show an enhanced response to visual stimuli in their receptive fields (Figure 7.45).

Most studies of attention using single-neuron recording and functional imaging have focused on the intraparietal area, especially the IPS and a subregion within the IPS, known in monkeys as the lateral intraparietal area, or LIP (Figure 7.46). This region is involved in saccadic eye movements and in visuospatial attention. To investigate what role LIP neurons play in visuospatial attention, James Bisley and Michael Goldberg (2006) collected intracranial recordings of LIP neurons from monkeys engaged in a discrimination task. The monkeys were to detect the properties of a stimulus at a covertly attended location to determine whether to execute a planned saccade toward that attended location. While the animal was covertly attending the cued location, occasional distractor stimuli appeared elsewhere. LIP neuronal activity when a distractor was present was compared with activity when there was no distractor. These results were also compared to the monkey's performance (i.e., its contrast detection threshold; Figure 7.47).

The best performance was observed when the target feature to be discriminated occurred in the location where LIP neuronal activity was higher. Put another way, if neuronal activity was highest at the attended location, performance was better for targets presented to that attended location. But if a distractor had been presented and neuronal activity had temporarily switched to be higher at another region of the LIP (corresponding to where the distractor was presented), then target discrimination was better at that (supposedly unattended) location.

For example, Figure 7.47 plots the results from one monkey. Right after the distractor appeared, probe performance was better (Figure 7.47 a; the red curve is below the blue curve) at the location of the distractor. But at about 400 ms (yellow shading), the curves cross. For the remainder of the plot the performance is better at the saccade target location (the blue curve is now below the red curve). These data tell us that the distractor briefly captured attention to its location (Figure 7.28), but then attention returned to the location of the saccade goal.

What were the neurons doing during this period? In Figure 7.47b, the red curve plots the neuronal responses evoked by the distractor stimulus, and the blue curve shows responses to the earlier saccade goal stimulus at the attended location. When the neuronal response to the distractor is larger than to the saccade goal stimulus, behavioral performance (Figure 7.47a) is better for the probe at the distractor location. But when the neuronal response to the distractor drops below that for the saccade goal stimulus, at about 400 ms, performance crosses back in favor of the attended location for probe discrimination.

Thus, by looking at the pattern of activity over the extent of the LIP, the researchers could actually predict the monkey's performance. By inference, they also could predict the momentary locus of the animal's visual attention. Bisley and Goldberg (2006) interpreted these findings as evidence that activity in the LIP provides a salience or priority map.

A salience map combines the maps of different individual features (color, orientation, movement, etc.) of a stimulus, resulting in an overall topographic map that shows how conspicuous a stimulus is, compared to those surrounding it (Koch & Ullman, 1985). This map is used by the oculomotor system as a saccade goal when a saccade is appropriate (i.e., when the stimulus is highly salient). At the same time, the visual system uses this map to determine the locus of attention. Thus, it appears that the LIP, which is an area of the parietal cortex and a component of the dorsal attention system, is concerned with the location and salience of objects. Let's now turn our attention to the ventral network.

## The Ventral Attention Network

So far, we have learned that the dorsal attention network is involved in focusing attention on items related to our current behavioral goals. Sometimes, however, the events that attract our attention away from a current goal should not be ignored. If a fire alarm goes off while you are intensely focused on reading and understanding this chapter, your attention should certainly shift to the alarm! According to Corbetta and his colleagues, this reaction to salient, unexpected, or novel stimuli is supported by the ventral attention network. While the dorsal attention network keeps you focused on this book, the ventral attention network is standing guard, ready to take over if any significant stimuli are detected in any sensory modality.

Corbetta took as a starting point the observation that neglect patients can detect a visual stimulus in the visual hemifield (left hemifield) opposite to their lesion (right hemisphere) when cued to its location, but if their attention is focused on a different location (right hemifield), then they are slow to respond and may even fail to detect an opposite-hemifield target. Structural imaging in patients has shown that in posterior cortex, such lesions are centered on the temporoparietal junction (TP J), the region at the border of the inferior parietal lobe and the posterior superior temporal lobe. In anterior cortex, lesions are typically located in regions more ventral to those of the dorsal attention network in the inferior and middle frontal gyri (Figure 7.48a). These findings suggested that these more ventral right-hemisphere regions may form a network that is critical for reorienting the focus of attention toward unexpected stimuli.

To test this model, Corbetta and his colleagues (2000) asked healthy participants to perform a task while their brain activity was imaged with fMRI. They used the spatial cuing tasks described earlier (Figure 7.15), where cues predict the most likely location of a subsequent target but are sometimes Incorrect, requiring participants to reorient attention to process the target at the unexpected location. Stimuli that appeared in unexpected locations activated the TPJ; in fact, the right T PJ responded equally to novel stimuli in both the right and left visual fields. The more dorsal intraparietal sulcus, however, was uniquely active when a cued location was attended and received maintained attention before the target was presented. These findings indicate that distinct parietal regions, and related cortical areas, mediate these different attentional properties.

Additional imaging studies conducted by the group identified what we now refer to as the ventral attention network. It is strongly lateralized to the right hemisphere, and it includes the TP J and the inferior and middle frontal gyri of the ventral frontal cortex (Figure 7.48b). Some have likened the activity of the ventral attention network to a circuit breaker, interrupting the current attentional focus established by the goal-directed dorsal network.

Of course, the dorsal and ventral networks interact with one another (Figure 7.48c). Corbetta and colleagues have suggested that the dorsal attention network, with its salience maps in the parietal cortex, provides the T PJ with behaviorally relevant information about stimuli, such as their visual salience. Together, the dorsal and ventral attention networks cooperate to make sure attention is focused on behaviorally relevant information, with the dorsal attention network focusing attention on relevant locations and potential targets, and the ventral attention network signaling the presence of salient, unexpected, or novel stimuli, enabling us to reorient the focus of our attention.

## Subcortical Components of Attentional Control Networks

Our discussion of attentional control of cortical mechanisms has been motivated to a great extent by evidence from patients with brain damage, such as those with neglect. In addition to cortical damage, though, subcortical damage is also well known to produce deficits in attention (Rafal and Posner, 1987) and clinical neglect (Karnath et al., 2004). What subcortical structures play significant roles in attentional control and selection in sensory cortex, and how do they contribute to our attentional abilities?

SUPERIOR COLLICULUS The superior colliculus is a midbrain structure made up of many layers of neurons, receiving direct inputs from the retina and other sensory systems, as well as from the basal ganglia and the cerebral cortex. As described in Chapter 2, there is one superior colliculus on each side of the midbrain, each receiving input primarily from the contralateral side of space. The superior colliculus projects multiple outputs to the thalamus and the motor system that, among other things, control the eye movements involved in changing the focus of overt attention. Since overt and covert attention are necessarily related, investigation of the role of the superior colliculus in covert attention has a long history.

In the early 1970s, Robert Wurtz and his colleagues discovered visually responsive neurons in the superior colliculus that were activated depending on how monkeys responded to stimuli. The neurons showed increased firing only when the animal was required both to attend to the location of the stimulus and also to prepare to make an eye movement toward the target. This finding led to the proposal that superior colliculus neurons do not participate in voluntary visual selective attention per se, but are merely part of the eye movement system, having a role in preparing to make overt eye movements to a location, but not in covert mechanisms of visual attention. While this is true for some neurons in the superior colliculus, it is not true for all neurons located there.

Desimone and colleagues (1990) investigated the role of the superior colliculus in attention by using deactivation methods in monkeys performing a target discrimination task. They injected a GABA agonist, which inhibits neuronal firing, into small zones of the superior colliculus corresponding to the receptive-field location of a target stimulus. They observed that this local deactivation of superior colliculus neurons reduced the animal's performance in discriminating the designated targets, but only if a distractor stimulus was also present somewhere in the visual field. This pattern led to speculation that the superior colliculus may indeed participate in covert attention (during distraction), as well as in overt eye movement planning.

William Newsome and his colleagues (Müller et al., 2005) performed a more direct test using electrical microstimulation. They trained monkeys to perform a task in which they had to detect and indicate the direction of motion of a small patch of dots in a larger array of flickering dots. The researchers then inserted an electrode to locally stimulate neurons in the superior colliculus. If they used a strong current, above the threshold that evokes an overt eye movement, they could see where the monkey's eyes moved (using eye-tracking methods) and thereby identify the region of visual space that those neurons coded. (Each superior colliculus contains a topographic map of the contralateral visual hemifield.) If they used a weak, subthreshold current, then the neurons were excited but an eye movement was not triggered.

They reasoned that if subthreshold stimulation mimicked the effects of covert attention on the neurons, then they should see improved discrimination of motion targets in the region coded by the neurons compared to other regions of the visual field (or to no stimulation at all). That, indeed, is what they found: The animal's performance at discriminating the direction of motion (left versus right) was improved when stimulated in the superior colliculus region corresponding to the visual field location of the motion target. It was as though the electrical stimulation had caused a shift of spatial attention to that location in space.

Gattass and Desimone (2014) replicated these basic findings in a different task and also showed that the superficial layers of the superior colliculus were more critical for this effect (the deep layers are more involved in saccadic eye movements). In addition, when monkeys were trained to attend a stimulus at one location and to ignore a distractor stimulus elsewhere in the visual field, stimulation of the superior colliculus at a site that corresponded to the distractor that was supposed to be ignored could cause a shift of attention to the distractor.

In humans, functional imaging has shown that the superior colliculus is activated during spatial attention. For example, Jin Fan and his colleagues have shown that the superior colliculus is activated along with structures of the dorsal attention network when participants orient covert spatial attention to an attention-directing cue (Xuan et al., 2016). Experimental studies in patients with damage to the superior colliculus also demonstrate a causal role in covert attention mechanisms.

Patients with degeneration of the superior colliculus (and parts of the basal ganglia) suffer from a disease known as progressive supranuclear palsy (PSP). When tested in cued attention paradigms, these patients have difficulty shifting their attention in response to a cue, and are slowed in responding to cued targets (especially in the vertical direction; Rafal et al., 1988). This pattern is different from that of patients with cortical lesions that cause neglect, whose most profound deficit in cued attention tasks is significant slowing in responding to uncued stimuli (invalid targets) when first cued elsewhere (in line with damage to the right ventral attention system).

Finally, the superior colliculus also appears to be involved in inhibitory processes during visual search. This association was demonstrated by a patient with a rare injury to one superior colliculus (Sapir et al., 1999). This patient had a reduced inhibition of return (IOR) for inputs to the lesioned colliculus. However, although the superior colliculus is involved in IOR, it in turn appears to depend on being activated by input from the frontal eye fields and the parietal cortex (parts of the dorsal network) in the hemisphere ipsilateral to the site of IOR (Ro et al., 2003). Taken together, the animal and human findings clearly identify the superior colliculus as a participant in the brain's attentional control mechanisms.

PULVINAR OF THE THALAMUS One of the many outputs from the superior colliculus goes to the inferior pulvinar. Located in a posterior region of the thalamus, the pulvinar (Figure 7.49) is composed of several cytoarchitectonically distinct subnuclei, each of which has specific inputs and outputs to cortical areas of all the lobes. For example, there are no direct connections to the ventrolateral pulvinar (VLP) from the parietal cortex, but there are with the dorsomedial pulvinar. Also, ventral-stream visual areas V1, V 2, V4, and IT project topographically to the VLP, which in turn sends projections back to these visual areas, forming a pulvinar— cortical loop. Each of these subnuclei performs different functions (Petersen et al., 1985), not all of which are understood.

The pulvinar has visually responsive neurons that exhibit selectivity for color, motion, and orientation, but it is not considered part of the classical retinogeniculostriate pathway of the visual hierarchy. That is, unlike the lateral geniculate nucleus of the thalamus, which serves as a relay between the retina and primary visual cortex, the pulvinar does not receive direct inputs from the retina. It does, however, receive visual inputs from the superior colliculus, which receives inputs from the retina, and these same pulvinar neurons project to specific cortical targets directly, such as motion area MT. Pulvinar neurons show enhanced activity when a stimulus is the target of a saccadic eye movement or when a stimulus is attended without eye movements toward the target. Thus, this structure may be involved in both voluntary and reflexive attention.

To figure out whether and how the pulvinar functions in attentional control, Steve Petersen, David Lee Robinson, and their colleagues investigated cells in the dorsomedial pulvinar, which has connections with the parietal cortex (Petersen et al., 1987, 1992). They administered microinjections of the GABA agonist muscimol, a drug that temporarily inhibits neuronal activity, to the pulvinar of monkeys to investigate how pulvinar inactivation would affect the animals' attentional ability (Figure 7.50). The monkeys' sensory processing remained intact, but they had difficulty orienting attention covertly to targets in the visual field contralateral to the injection, as well as difficulty filtering distracting information. When competing distractors were present in the visual field, they had difficulty discriminating color or form.

These attention impairments are similar to what is seen with parietal cortex deactivation. Petersen and colleagues also showed that when a different drug, the GABA antagonist bicuculline, was administered, the monkeys readily directed their attention covertly to contralesional targets. Hence, cells in the dorsomedial pulvinar appear to play a major role in covert spatial attention and the filtering of stimuli, presumably via its interactions with portions of the parietal cortex that are part of the dorsal attentional control network.

In general, these findings mesh with the findings from patients with lesions to the dorsal portion of the pulvinar, who have difficulty engaging attention at a cued location. Compared with normal participants, their reaction times are increased for both validly cued and invalidly cued targets that appear in the contralesional space. This condition stands in contrast to patients with cortical lesions of the inferior parietal and temporoparietal junction. Their main deficit is greatly increased reaction time to invalid targets in the contralesional space but not to valid targets (Figure 7.51).

Other regions of the pulvinar also play a role in attention modulation in visual cortex. Yuri Saalmann, Sabine Kastner, and their colleagues (Saalmann et al., 2012) were interested in the VLP and its feedforward and feedback connection loops between higher-order visual cortices V4 and the temporo-occipital area, or TEO, a higher-order visual area located just anterior to V 4 in the ventral visual pathway (Figure 7.52).

Because of the reciprocal connections with the ventral visual stream, the researchers thought the VLP might play a part in synchronizing processing across the visual cortex during selective attention. Ihey ran a study in monkeys, first using diffusion tensor imaging (DTI) to identify the regions of the pulvinar that were anatomically interconnected with V4 and TEO. Then they recorded neuronal firing and local field potentials simultaneously in the three structures while the animals performed a cued spatial attention task. They found that VLP neurons that coded the cued region of space showed higher firing rates both during the cue-to-target interval and in response to the target when it appeared. They also found an increase in the synchrony between pulvinar neurons during focused attention.

The researchers wondered whether this pulvinar activity was influencing how the visual cortex was processing attended versus ignored information. To investigate this question, they reasoned that because of the pulvinar's interconnections with V4 and TEO, they should record the local field potentials in these regions and then look for evidence that attention influences the V4-to-TEO synchrony. They found it: Synchrony was higher in the alpha (8—15 Hz) frequency range and in the 30 to 60-Hz band of the gamma frequency ranges (Figure 7.53). Further analyses provided compelling evidence that the pulvinar modulates the synchronization between cortical areas according to the direction of spatial attention.

Huihui Zhou, Robert Schafer, and Robert Desimone (2016) directly tested the causal influence of the VLP on cortical responses by reversibly deactivating it with muscimol. Then they recorded from corresponding V4 and IT neurons before and after the inactivation in the affected and nonaffected portions of the visual field to see the effects as monkeys performed a spatial attention task. They found that in the sites not affected by muscimol, no changes were observed in the monkeys' task performance. But in the regions of space affected by the deactivated pulvinar, the animals were profoundly impaired at performing the task. The researchers also found that pulvinar deactivation reduced the effects of attention on neuronal synchronization. This finding provides additional evidence that the pulvinar plays a critical role in attentional control by coordinating the synchronous activity of interconnected brain regions in the ventral visual pathway.

Given what we have learned about the role of the dorsal attention network (frontal and parietal cortex) in top-down attentional control, the work studying the pulvinar suggests that frontal and parietal cortex exerts control over sensory processing via interactions with the pulvinar. However, the precise mechanisms of the interactions between top-down cortical control over attention and the pulvinar remain incompletely understood.

## TAKE-HOME MESSAGES 

- Current evidence suggests that two separate frontoparietal cortical systems direct different attentional control operations during orienting: a dorsal attention network, concerned primarily with orienting attention, and a ventral attention network, concerned with the nonspatial aspects of attention and alerting. The two systems interact and cooperate to produce normal behavior.
- The dorsal frontoparietal attention network is bilateral and includes the superior frontal cortex, inferior parietal cortex (located in the posterior parietal lobe), superior temporal cortex, and portions of the posterior cingulate cortex and insula.
- The ventral network is strongly lateralized to the right hemisphere and includes the posterior parietal cortex of the temporoparietal junction (TPJ) and the ventral frontal cortex (VFC) made up of the inferior and middle frontal gyri.
- In addition, there are subcortical networks that include the superior colliculi and the pulvinar of the thalamus.

---

# Summary

In this chapter we looked at key aspects of attentional mechanisms and examined the goal-directed, top-down control systems of attention and how they influence perceptual processing, as well as how attention can be captured by events in the sensory world. The picture we find is of distributed but highly specific brain systems participating in attentional control. The roles and limits of these systems in attention are becoming more clearly defined as we combine attentional theory, experimental and cognitive psychological findings, and neurophysiological approaches in healthy participants and patients with brain damage.

Systems for controlling attention include portions of the parietal lobe, temporal cortex, frontal cortex, and subcortical structures; these constitute the sources of attentional control. The result in visual processing—which has been our example system—is that, in the sensory pathways, we observe modulations in the activity of neurons as they analyze and encode perceptual information as a function of their relevance. These areas affected by attention are the sites of attentional selection.

We no longer wonder whether early or late selection is the mechanism for selective attention, because we now know that attention can operate at multiple stages of processing, including subcortical stages of the sensory pathways. The fascinating fact is that physical stimuli that impinge on our sensory receptors may not be expressed in our awareness, either at the time they occur or later via our recollections. The interaction of stimulus salience and goaldirected attention determines which inputs reach awareness and which do not.

Attentional phenomena are diverse and entail many brain computations and mechanisms. When these are compromised by damage or disease, the results can be devastating for the individual. Cognitive neuroscience is vigorously carving away at the physiological and computational under-pinnings of these phenomena, with the dual goals of providing a complete account of how the healthy brain functions, and shedding light on how to ameliorate attentional deficits in all their forms.